{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os, sys\n",
    "from models import Model_GNNs\n",
    "from dataclasses import dataclass\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from DataPipeline.preprocessing import node_encoder\n",
    "\n",
    "from torch_geometric.data import Batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experiment:\n",
    "    exp_name: str\n",
    "    encod: str\n",
    "    keku: bool\n",
    "    train: bool\n",
    "    encoding_size: int\n",
    "    edge_size: int\n",
    "    encoding_option: str = 'charged'\n",
    "\n",
    "exp = Experiment('GNN_baseline_3_modif', 'charged', True, False, 13, 3)\n",
    "exp2 = Experiment('GNN_baseline_3_modif_debiased', 'charged', True, False, 13, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_current_nodes_batched(current_node_tensor, graph_batch):\n",
    "    # Get the unique graph ids\n",
    "    batch = graph_batch.batch\n",
    "    unique_graph_ids = torch.unique(batch)\n",
    "    # Create a 2D mask that shows where each graph's nodes are located in batch\n",
    "    mask = batch[None, :] == unique_graph_ids[:, None]\n",
    "    # Compute the cumulative sum of the mask along the second dimension\n",
    "    cumulative_mask = mask.cumsum(dim=1)\n",
    "    # Now, for each graph, the nodes are numbered from 1 to N (or 0 to N-1 if we subtract 1)\n",
    "    node_indices_per_graph = cumulative_mask - 1\n",
    "    # But we only want the indices of certain nodes (specified by current_node_tensor)\n",
    "    # So we create a mask that is True where the node index equals the current node index for the graph\n",
    "    current_node_mask = node_indices_per_graph == current_node_tensor[:, None]\n",
    "    # The result is the indices in batch where current_node_mask is True\n",
    "    # Find the arg of the first True in each row\n",
    "    result = torch.argmax(current_node_mask.int(), dim=1)\n",
    "    return result\n",
    "\n",
    "def set_last_nodes(batch, last_prediction_size, encoding_size):\n",
    "    # Reset the current node column\n",
    "    batch.x[:, encoding_size - 1] = 0\n",
    "    # Set the last nodes to 1\n",
    "    batch.x[batch.x.shape[0] - last_prediction_size:, encoding_size - 1] = 1\n",
    "    return batch\n",
    "\n",
    "def increment_feature_position(batch, current_nodes_batched, stop_mask, encoding_size):\n",
    "\n",
    "    stopped_current_nodes = current_nodes_batched[stop_mask]\n",
    "\n",
    "    batch.x[stopped_current_nodes, encoding_size] = 1\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_mask(batch_graph, current_nodes_tensor : torch.tensor, last_prediction_size, encoding_size):\n",
    "    # Create a mask for the current nodes tensor    \n",
    "    feature_postion = batch_graph.x[:, encoding_size]\n",
    "    mask = torch.logical_not(feature_postion.bool())\n",
    "    # Set the last nodes to False\n",
    "    mask[batch_graph.x.shape[0] - last_prediction_size:] = False\n",
    "    # Set the current nodes to False\n",
    "    mask[current_nodes_tensor] = False\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "def select_node_batch(prediction, batch_data, edge_size, mask):\n",
    "\n",
    "    # Sum on the first dimensions of each vector\n",
    "    sum_on_first_dims = prediction[:, :edge_size - 1].sum(dim=1)\n",
    "    unique_graph_ids = torch.unique(batch_data)\n",
    "    expanded_sum = sum_on_first_dims[None, :].expand(unique_graph_ids.shape[0], -1)\n",
    "    \n",
    "    # Create a 2D mask that shows where each graph's nodes are located in batch\n",
    "    mask_location = batch_data[None, :] == unique_graph_ids[:, None]\n",
    "    # Apply the mask to each row of mask_location\n",
    "    mask = mask_location * mask[None, :]\n",
    "    # Apply mask to the sum tensor, setting masked values to -inf\n",
    "    masked_sum = expanded_sum.masked_fill(~mask,float ('-inf'))\n",
    "    #Find the max value in each row\n",
    "    max_indices = torch.argmax(masked_sum, dim=1)\n",
    "    # Create a count mask that counts how many True values there are in each row\n",
    "    count_mask = mask.sum(dim=1)\n",
    "\n",
    "    # Replace indices where there were no True values in the mask with -1 (or any value you want)\n",
    "    minus_one = torch.tensor([-1], device=max_indices.device)\n",
    "    max_indices = max_indices.where(count_mask > 0, minus_one)\n",
    "    # Sample using the tensor using the multinomial function\n",
    "    sampled_indices = prediction.multinomial(num_samples=1).squeeze()\n",
    "    # Replace -1 values in max_indices with the corresponding sampled_indices\n",
    "    final_indices = torch.where(max_indices != -1, sampled_indices[max_indices], prediction.size(1) - 1)\n",
    "\n",
    "    return final_indices, max_indices\n",
    "\n",
    "\n",
    "def select_option_batch(choice_input_softmax, sigmoid_input):\n",
    "    # Sample with a multinomial distribution  \n",
    "    choice_sampled = torch.multinomial(choice_input_softmax, num_samples=1)\n",
    "\n",
    "    # Get the sigmoid values for the chosen nodes\n",
    "    chosen_sigmoid_values = sigmoid_input[choice_sampled.squeeze()]\n",
    "    # Generate random numbers for comparison\n",
    "    random_numbers = torch.rand(chosen_sigmoid_values.shape, device=chosen_sigmoid_values.device)\n",
    "    # Binary decision based on the sigmoid value: If the random number is less than the sigmoid value, choose 1\n",
    "    decision = (random_numbers < chosen_sigmoid_values).to(torch.long)\n",
    "\n",
    "    return choice_sampled, decision\n",
    "\n",
    "def compute_softmax_GNN3(choice_input, batch_data, mask):\n",
    "    unique_graph_ids = torch.unique(batch_data)\n",
    "    mask_location = batch_data[None, :] == unique_graph_ids[:, None]\n",
    "\n",
    "    # Multiply the softmax and the mask location\n",
    "    choice_masked = choice_input[None, :] * mask_location\n",
    "    choice_masked = choice_masked * mask[None, :]\n",
    "    choice_masked = choice_masked.masked_fill(torch.logical_or(~mask_location, ~mask[None, :]), float('-inf'))\n",
    "\n",
    "    # Apply softmax to the masked tensor\n",
    "    choice_softmax = F.softmax(choice_masked, dim=1)\n",
    "    # If all values in a row are nan (due to softmax of -inf), replace that row with uniform distribution\n",
    "    row_is_all_nan = torch.isnan(choice_softmax).all(dim=1)\n",
    "    default_values = torch.ones_like(choice_softmax[0, :]) / choice_softmax.size(1)  # uniform distribution\n",
    "    choice_softmax[row_is_all_nan, :] = default_values\n",
    "\n",
    "    return choice_softmax\n",
    "    \n",
    "\n",
    "def add_nodes_and_edges_batch(batch, new_nodes, new_edges, current_nodes_batched, mask):\n",
    "    device = batch.x.device\n",
    "    # Add one zero to the end of the new_nodes\n",
    "    new_nodes = torch.cat([new_nodes, torch.zeros(new_nodes.size(0), 1, device=new_nodes.device)], dim=1)\n",
    "    # Add new nodes to the node attributes, considering the mask\n",
    "    new_nodes_masked = new_nodes[mask]\n",
    "\n",
    "    batch.x = torch.cat([batch.x, new_nodes_masked], dim=0)\n",
    "\n",
    "    # Create new edges between the current nodes and the new nodes in a bidirectional manner\n",
    "    num_new_nodes = new_nodes_masked.shape[0]\n",
    "    new_edge_indices = torch.stack([current_nodes_batched[mask], torch.arange(batch.num_nodes - num_new_nodes, batch.num_nodes, device=device)], dim=0)\n",
    "    new_edge_indices = torch.cat([new_edge_indices, new_edge_indices.flip(0)], dim=1)  # Making the edges bidirectional\n",
    "\n",
    "    # Adding the new edges to the edge_index\n",
    "    batch.edge_index = torch.cat([batch.edge_index, new_edge_indices], dim=1)\n",
    "\n",
    "    # Create new edge attributes and mask out the entries where mask is False\n",
    "    new_edge_attrs = new_edges[mask].repeat(2, 1)\n",
    "\n",
    "    # Adding the new edge attributes to the edge attributes\n",
    "    batch.edge_attr = torch.cat([batch.edge_attr, new_edge_attrs], dim=0)\n",
    "\n",
    "    # Update the batch.batch tensor\n",
    "    new_batch_entries = torch.arange(new_nodes.shape[0], device=device)\n",
    "    batch.batch = torch.cat([batch.batch, new_batch_entries[mask]])\n",
    "    #check_valence(batch, torch.arange(batch.num_nodes - num_new_nodes, batch.num_nodes, device=device).tolist())\n",
    "    #check_valence(batch, current_nodes_batched[mask].tolist())\n",
    "    # Return the updated batch\n",
    "    return batch\n",
    "\n",
    "def add_edges_and_attributes(batch, edges_predicted, indices, mask, stopping_mask):\n",
    "\n",
    "    num_new_edges = edges_predicted.shape[0]\n",
    "    mask_edge_predicted = edges_predicted[mask]\n",
    "    mask_indices = indices[mask]\n",
    "    \n",
    "    num_new_nodes = torch.sum(stopping_mask)\n",
    "    last_indices = torch.arange(batch.num_nodes - num_new_nodes, batch.num_nodes, device=batch.x.device)\n",
    "\n",
    "    last_nodes_batch = torch.full(stopping_mask.shape, -1, device=batch.x.device)\n",
    "\n",
    "    last_nodes_batch[stopping_mask] = last_indices\n",
    "    new_edges_indices = torch.stack([mask_indices, last_nodes_batch[mask]], dim=0)\n",
    "    new_edges_indices = torch.cat([new_edges_indices, new_edges_indices.flip(0)], dim=1)  # Making the edges bidirectional\n",
    "\n",
    "    # Adding the new edges to the edge_index\n",
    "    batch.edge_index = torch.cat([batch.edge_index, new_edges_indices], dim=1)\n",
    "\n",
    "    # Create new edge attributes and mask out the entries where mask is False\n",
    "    new_edge_attrs = mask_edge_predicted.repeat(2, 1)\n",
    "\n",
    "    # Adding the new edge attributes to the edge attributes\n",
    "    batch.edge_attr = torch.cat([batch.edge_attr, new_edge_attrs], dim=0)\n",
    "\n",
    "    #check_valence(batch, mask_indices.tolist())\n",
    "    #check_valence(batch, last_indices.tolist())\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "def sample_first_atom_batch(batch_size, encoding = 'reduced'):\n",
    "    if encoding == 'reduced' or encoding == 'charged':\n",
    "        prob_dict = {'60': 0.7385023585929047, \n",
    "                    '80': 0.1000143018658728, \n",
    "                    '70': 0.12239949901813525, \n",
    "                    '90': 0.013786373862576426, \n",
    "                    '160': 0.017856330814654413,\n",
    "                    '170': 0.007441135845856433}\n",
    "    if encoding == 'polymer':\n",
    "        prob_dict = {'60': 0.7489344573582472,\n",
    "                    '70': 0.0561389266682314,\n",
    "                    '80': 0.0678638375933265,\n",
    "                    '160': 0.08724385192820308,\n",
    "                    '90': 0.032130486119902095,\n",
    "                    '140': 0.007666591133009364,\n",
    "                    '150': 2.184919908044154e-05}\n",
    "\n",
    "    atoms = [random.choices(list(prob_dict.keys()), weights=list(prob_dict.values()))[0] for _ in range(batch_size)]\n",
    "    return atoms\n",
    "\n",
    "def create_torch_graph_from_one_atom_batch(atoms, edge_size, encoding_option='reduced') -> list:\n",
    "    graphs = []\n",
    "    for atom in atoms:\n",
    "        num_atom = int(atom)\n",
    "        atom_attribute = node_encoder(num_atom, encoding_option=encoding_option)\n",
    "        # Create graph\n",
    "        # Increase the size of atom_attribute by one \n",
    "        atom_attribute = torch.cat((atom_attribute, torch.zeros(1)), dim=0)\n",
    "\n",
    "        graph = torch_geometric.data.Data(x=atom_attribute.view(1, -1), edge_index=torch.empty((2, 0), dtype=torch.long), edge_attr=torch.empty((0, edge_size)))\n",
    "        graphs.append(graph)\n",
    "            \n",
    "    \n",
    "    return Batch.from_data_list(graphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling_Path_Batch():\n",
    "    def __init__(self, GNNs_Models_q, GNNs_Models_a, GNNs_Models_pi, features, lambdas, device, batch_size, args):\n",
    "        \"\"\"\n",
    "        Initialize the sampling path batch\n",
    "        input:\n",
    "        GNNs_Models_q: list of the GNNs models for the q function\n",
    "        GNNs_Models_a: list of the GNNs models for the a function\n",
    "        GNNs_Models_pi: list of the GNNs models for the pi function\n",
    "        features: list of the features of the molecules\n",
    "        lambdas: list of the lambdas of the molecules\n",
    "        batch_size: size of the batch\n",
    "        device: device to use for the computation\n",
    "\n",
    "        return: None\n",
    "        \"\"\"\n",
    "        self.GNNs_Models_q = GNNs_Models_q\n",
    "        self.GNNs_Models_a = GNNs_Models_a\n",
    "        self.GNNs_Models_pi = GNNs_Models_pi\n",
    "        self.features = features\n",
    "        self.lambdas = lambdas\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.encoding_size = args.encoding_size\n",
    "        self.encoding_option = args.encoding_option\n",
    "        self.edge_size = args.edge_size\n",
    "\n",
    "        batch_mol_graph = create_torch_graph_from_one_atom_batch(sample_first_atom_batch(batch_size = batch_size, encoding = self.encoding_option), edge_size=self.edge_size, encoding_option=self.encoding_option)\n",
    "        self.batch_mol_graph = batch_mol_graph.to(device) # Encoded in size 14 for the feature position\n",
    "        self.queues = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "        self.node_counts = torch.zeros(batch_size, dtype=torch.long, device=device)\n",
    "        self.finished_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "        # Float 64 for the q_value, a_value and pi_value\n",
    "        # Requires grad for the pi_value\n",
    "\n",
    "        self.a_value = torch.ones(batch_size, dtype=torch.float64, device=device)\n",
    "        self.q_value = torch.ones(batch_size, dtype=torch.float64, device=device)\n",
    "        self.pi_value = torch.ones(batch_size, dtype=torch.float64, device=device, requires_grad=True)\n",
    "\n",
    "    def one_step(self):\n",
    "\n",
    "        current_nodes = self.queues.clone()\n",
    "        \n",
    "        # Get the molecules that are finished which means that the number in the queue is superior to the number of nodes in the molecule\n",
    "        old_finished_mask = self.finished_mask\n",
    "        self.finished_mask = current_nodes > self.node_counts\n",
    "\n",
    "        # test if no molecule has changed from finished to unfinished\n",
    "        if torch.any(old_finished_mask & ~self.finished_mask):\n",
    "            print('error')\n",
    "            raise ValueError('error')\n",
    "        # If all the mask is True, then all the molecules are finished\n",
    "        if torch.all(self.finished_mask):\n",
    "            return \n",
    "        \"\"\"\n",
    "        Prepare the GNN 1 BATCH\n",
    "        \"\"\"\n",
    "        current_nodes[self.finished_mask] = current_nodes[self.finished_mask] - 1\n",
    "        current_nodes_batched = return_current_nodes_batched(current_nodes, self.batch_mol_graph) #reindex the nodes to match the batch size\n",
    "        \n",
    "        self.batch_mol_graph.x[: , self.encoding_size - 1] = 0\n",
    "        # Do you -1 for the current nodes that are finished\n",
    "\n",
    "        self.batch_mol_graph.x[current_nodes_batched, self.encoding_size - 1] = 1\n",
    "\n",
    "        # Add score features if needed\n",
    "\n",
    "\n",
    "\n",
    "        q_predictions = self.GNNs_Models_q.GNN1_model(self.batch_mol_graph)\n",
    "        a_predictions = self.GNNs_Models_a.GNN1_model(self.batch_mol_graph)\n",
    "        pi_predictions = self.GNNs_Models_pi.GNN1_model(self.batch_mol_graph)\n",
    "\n",
    "        \n",
    "        # Apply softmax to prediction\n",
    "        q_softmax_predictions = F.softmax(q_predictions, dim=1)\n",
    "        a_softmax_predictions = F.softmax(a_predictions, dim=1)\n",
    "        pi_softmax_predictions = F.softmax(pi_predictions, dim=1)\n",
    "\n",
    "        # Sample next node from prediction\n",
    "        predicted_nodes = torch.multinomial(q_softmax_predictions, num_samples=1)\n",
    "\n",
    "        # Get the q, a and pi values for the predicted_node \n",
    "        q_value = q_softmax_predictions[torch.arange(self.batch_size), predicted_nodes.flatten()]\n",
    "        a_value = a_softmax_predictions[torch.arange(self.batch_size), predicted_nodes.flatten()]\n",
    "        pi_value = pi_softmax_predictions[torch.arange(self.batch_size), predicted_nodes.flatten()]\n",
    "\n",
    "        # Actualize the q, a, pi and use the finished mask to only actualize the values of the molecules that are not finished\n",
    "        self.q_value = self.q_value * torch.max(self.finished_mask.float(), q_value)\n",
    "        self.a_value = self.a_value * torch.max(self.finished_mask.float(), a_value)\n",
    "        self.pi_value = self.pi_value * torch.max(self.finished_mask.float(), pi_value)\n",
    "\n",
    "        # Create a mask for determining which graphs should continue to GNN2\n",
    "        mask_gnn2 = (predicted_nodes != self.encoding_size - 1).flatten()\n",
    "\n",
    "        # Handle the stopping condition (where predicted_node is encoding_size - 1)\n",
    "        stop_mask = (predicted_nodes == self.encoding_size - 1).flatten()\n",
    "            \n",
    "        # Increment the node count for graphs that haven't stopped and that are not finished\n",
    "        self.node_counts = self.node_counts + torch.logical_and(~stop_mask, ~self.finished_mask).long()\n",
    "\n",
    "        # Increment the queue for graphs that have been stopped and that are not finished\n",
    "        \n",
    "        self.queues = self.queues + torch.logical_and(stop_mask, ~self.finished_mask).long()\n",
    "\n",
    "        # Increment the feature position for graphs that have been stopped\n",
    "\n",
    "        self.batch_mol_graph = increment_feature_position(self.batch_mol_graph, current_nodes_batched, stop_mask, self.encoding_size)\n",
    "\n",
    "        # Encode next node for the entire batch\n",
    "        encoded_predicted_nodes = torch.zeros(q_predictions.size(), device=self.device, dtype=torch.float)\n",
    "        encoded_predicted_nodes.scatter_(1, predicted_nodes, 1)\n",
    "\n",
    "        #GNN2 \n",
    "                \n",
    "        # add zeros to the neighbor because of the feature position\n",
    "        encoded_predicted_nodes = torch.cat([encoded_predicted_nodes, torch.zeros(self.batch_size, 1).to(encoded_predicted_nodes.device)], dim=1)\n",
    "\n",
    "        self.batch_mol_graph.neighbor = encoded_predicted_nodes\n",
    "\n",
    "        \n",
    "        q_predictions_2 = self.GNNs_Models_q.GNN2_model(self.batch_mol_graph)\n",
    "        a_predictions_2 = self.GNNs_Models_a.GNN2_model(self.batch_mol_graph)\n",
    "        pi_predictions_2 = self.GNNs_Models_pi.GNN2_model(self.batch_mol_graph)\n",
    "\n",
    "        # Apply softmax to prediction\n",
    "        q_softmax_predictions_2 = F.softmax(q_predictions_2, dim=1)\n",
    "        a_softmax_predictions_2 = F.softmax(a_predictions_2, dim=1)\n",
    "        pi_softmax_predictions_2 = F.softmax(pi_predictions_2, dim=1)\n",
    "\n",
    "        predicted_edges = torch.multinomial(q_softmax_predictions_2, num_samples=1)\n",
    "\n",
    "        # Get the q, a and pi values for the predicted_node\n",
    "        q_value = q_softmax_predictions_2[torch.arange(self.batch_size), predicted_edges.flatten()]\n",
    "        a_value = a_softmax_predictions_2[torch.arange(self.batch_size), predicted_edges.flatten()]\n",
    "        pi_value = pi_softmax_predictions_2[torch.arange(self.batch_size), predicted_edges.flatten()]\n",
    "\n",
    "\n",
    "\n",
    "        # Actualize the q, a, pi and use the mask to only actualize the graphs that have not stopped by getting the max of the stop mask, the finis\n",
    "        self.q_value = self.q_value * torch.max(torch.logical_or(stop_mask, self.finished_mask).float(), q_value)\n",
    "        self.a_value = self.a_value * torch.max(torch.logical_or(stop_mask, self.finished_mask).float(), a_value)\n",
    "        self.pi_value = self.pi_value * torch.max(torch.logical_or(stop_mask, self.finished_mask).float(), pi_value)\n",
    "\n",
    "    \n",
    "        encoded_predicted_edges = torch.zeros_like(q_predictions_2, device=self.device, dtype=torch.float)\n",
    "        encoded_predicted_edges.scatter_(1, predicted_edges, 1)\n",
    "        \n",
    "        # Create a new node that is going to be added to the graph for each batch\n",
    "        new_nodes = torch.zeros(self.batch_size, self.encoding_size, device=self.device, dtype=torch.float)\n",
    "        new_nodes.scatter_(1, predicted_nodes, 1)\n",
    "\n",
    "        #GNN3\n",
    "\n",
    "        # Add the node and the edge to the graph\n",
    "        self.batch_mol_graph = add_nodes_and_edges_batch(self.batch_mol_graph, new_nodes, encoded_predicted_edges, current_nodes_batched, torch.logical_and(mask_gnn2, ~self.finished_mask))\n",
    "        \n",
    "\n",
    "        self.batch_mol_graph = set_last_nodes(self.batch_mol_graph, torch.sum(torch.logical_and(mask_gnn2, ~self.finished_mask), dim=0), self.encoding_size)       \n",
    "        mask = create_mask(self.batch_mol_graph, current_nodes_tensor = current_nodes_batched, last_prediction_size=torch.sum(torch.logical_and(mask_gnn2, ~self.finished_mask), dim=0), encoding_size=self.encoding_size)\n",
    "        self.batch_mol_graph.mask = mask\n",
    "    \n",
    "\n",
    "        q_prediction_closing = self.GNNs_Models_q.GNN3_1_model(self.batch_mol_graph)\n",
    "        a_prediction_closing = self.GNNs_Models_a.GNN3_1_model(self.batch_mol_graph)\n",
    "        pi_prediction_closing = self.GNNs_Models_pi.GNN3_1_model(self.batch_mol_graph)\n",
    "        \n",
    "        q_sigmoid_prediction_3 = torch.sigmoid(q_prediction_closing).flatten()\n",
    "        a_sigmoid_prediction_3 = torch.sigmoid(a_prediction_closing).flatten()\n",
    "        pi_sigmoid_prediction_3 = torch.sigmoid(pi_prediction_closing).flatten()\n",
    "\n",
    "        random_number = torch.rand(q_sigmoid_prediction_3.shape, device=self.device)\n",
    "\n",
    "        closing_mask = (random_number < q_sigmoid_prediction_3).flatten() \n",
    "\n",
    "        unique_graph_ids = torch.unique(self.batch_mol_graph.batch)\n",
    "        mask_location = self.batch_mol_graph.batch[None, :] == unique_graph_ids[:, None]\n",
    "\n",
    "        mask_location =  mask_location * mask[None, :]\n",
    "        # Create a mask if all the lines of mask_location are false then put false in the closing mask\n",
    "        mask_location = torch.sum(mask_location, dim=1)\n",
    "\n",
    "\n",
    "        # Get the q, a and pi values for the predicted_node. If the closing mask is true, the value is sigmoid, otherwise it is 1-sigmoid\n",
    "        q_values = torch.where(closing_mask, q_sigmoid_prediction_3, 1 - q_sigmoid_prediction_3)\n",
    "        a_values = torch.where(closing_mask, a_sigmoid_prediction_3, 1 - a_sigmoid_prediction_3)\n",
    "        pi_values = torch.where(closing_mask, pi_sigmoid_prediction_3, 1 - pi_sigmoid_prediction_3)\n",
    "\n",
    "\n",
    "        # Actualize the q, a, pi and use the mask to only actualize the graphs that have not stopped by getting the max of the stop mask, the finis\n",
    "        self.q_value = self.q_value * torch.max(torch.logical_or(stop_mask, self.finished_mask).float(), q_values)\n",
    "        self.a_value = self.a_value * torch.max(torch.logical_or(stop_mask, self.finished_mask).float(), a_values)\n",
    "        self.pi_value = self.pi_value * torch.max(torch.logical_or(stop_mask, self.finished_mask).float(), pi_values)\n",
    "        \n",
    "\n",
    "        q_chosing_prediction = self.GNNs_Models_q.GNN3_2_model(self.batch_mol_graph)\n",
    "        a_chosing_prediction = self.GNNs_Models_a.GNN3_2_model(self.batch_mol_graph)\n",
    "        pi_chosing_prediction = self.GNNs_Models_pi.GNN3_2_model(self.batch_mol_graph)\n",
    "        \n",
    "        \n",
    "        q_choice_input = q_chosing_prediction[:, 1]\n",
    "        q_sigmoid_input = q_chosing_prediction[:, 0]\n",
    "        a_choice_input = a_chosing_prediction[:, 1]\n",
    "        a_sigmoid_input = a_chosing_prediction[:, 0]\n",
    "        pi_choice_input = pi_chosing_prediction[:, 1]\n",
    "        pi_sigmoid_input = pi_chosing_prediction[:, 0]\n",
    "\n",
    "        # Apply sigmoid \n",
    "\n",
    "        q_sigmoid_input = torch.sigmoid(q_sigmoid_input)\n",
    "        a_sigmoid_input = torch.sigmoid(a_sigmoid_input)\n",
    "        pi_sigmoid_input = torch.sigmoid(pi_sigmoid_input)\n",
    "\n",
    "        # Compute softmax \n",
    "\n",
    "        q_softmax_input = compute_softmax_GNN3(q_choice_input, self.batch_mol_graph.batch, mask)\n",
    "        a_softmax_input = compute_softmax_GNN3(a_choice_input, self.batch_mol_graph.batch, mask)\n",
    "        pi_softmax_input = compute_softmax_GNN3(pi_choice_input, self.batch_mol_graph.batch, mask)\n",
    "\n",
    "\n",
    "        choosen_indexes, decision = select_option_batch(q_softmax_input, q_sigmoid_input)\n",
    "\n",
    "        choosen_indexes = choosen_indexes.squeeze()\n",
    "        \n",
    "        # Get the q, a and pi values for the predicted_node. Based on the choosen index.\n",
    "\n",
    "        # Extract the choosen input from sigmoid\n",
    "\n",
    "        q_extracted_sigmoid = q_sigmoid_input[choosen_indexes].squeeze()\n",
    "        a_extracted_sigmoid = a_sigmoid_input[choosen_indexes].squeeze()\n",
    "        pi_extracted_sigmoid = pi_sigmoid_input[choosen_indexes].squeeze()\n",
    "\n",
    "        decision = decision.squeeze()\n",
    "\n",
    "        graph_indices = torch.arange(q_softmax_input.shape[0], device=self.device)\n",
    "        \n",
    "        q_values = (q_softmax_input[graph_indices, choosen_indexes].squeeze()) * torch.where(decision == 1, q_extracted_sigmoid, 1 - q_extracted_sigmoid)\n",
    "        a_values = (a_softmax_input[graph_indices, choosen_indexes].squeeze()) * torch.where(decision == 1, a_extracted_sigmoid, 1 - a_extracted_sigmoid)\n",
    "        pi_values = (pi_softmax_input[graph_indices, choosen_indexes].squeeze()) * torch.where(decision == 1, pi_extracted_sigmoid, 1 - pi_extracted_sigmoid)\n",
    "        \n",
    "        # Actualize the q, a, pi and use the mask to only actualize the graphs that have not stopped by getting the max of the stop mask, the finis\n",
    "        total_mask = torch.logical_or(torch.logical_or(~closing_mask, stop_mask), self.finished_mask)\n",
    "\n",
    "        self.q_value = self.q_value * torch.max(total_mask.float(), q_values)\n",
    "        self.a_value = self.a_value * torch.max(total_mask.float(), a_values)\n",
    "        self.pi_value = self.pi_value * torch.max(total_mask.float(), pi_values)\n",
    "\n",
    "\n",
    "        encoded_edges_predicted = torch.zeros((decision.shape[0], self.edge_size), device=self.device, dtype=torch.float)\n",
    "\n",
    "        encoded_edges_predicted.scatter_(1, decision.unsqueeze(1), 1)\n",
    "\n",
    "        total_mask = torch.logical_and(torch.logical_and(closing_mask, mask_gnn2), ~self.finished_mask)\n",
    "\n",
    "        self.mol_graphs_list = add_edges_and_attributes(self.batch_mol_graph, encoded_edges_predicted, choosen_indexes.flatten(), total_mask, torch.logical_and(mask_gnn2, ~self.finished_mask))\n",
    "\n",
    "\n",
    "    def full_generation(self):\n",
    "        max_iter = 150\n",
    "        i = 0\n",
    "        while torch.all(self.finished_mask) == False:\n",
    "            if i > max_iter:\n",
    "                break\n",
    "            self.one_step()\n",
    "            i += 1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint number 1 of the epoch 2050.0 with a loss of 0.5107812829972882\n",
      "Loading best checkpoint number 1 of the epoch 2550.0 with a loss of 0.1984780294417413\n",
      "Loading best checkpoint number 2 of the epoch 2700.0 with a loss of 10.3904066518488\n",
      "..\\trained_models\\GNN_baseline_3_modif\\GNN1_baseline\\history_training\\checkpoint_1.pt ..\\trained_models\\GNN_baseline_3_modif\\GNN2_baseline\\history_training\\checkpoint_1.pt ..\\trained_models\\GNN_baseline_3_modif\\GNN3_split_two_without_node_embedding\\history_training\\checkpoint_2.pt\n",
      "Loading best checkpoint number 1 of the epoch 2950.0 with a loss of 0.5103764619447081\n",
      "Loading best checkpoint number 2 of the epoch 2150.0 with a loss of 0.19804639930989767\n",
      "Loading best checkpoint number 2 of the epoch 2700.0 with a loss of 10.3904066518488\n",
      "..\\trained_models\\GNN_baseline_3_modif_debiased\\GNN1_charged_baseline_debiased\\history_training\\checkpoint_1.pt ..\\trained_models\\GNN_baseline_3_modif_debiased\\GNN2_charged_baseline_debiased\\history_training\\checkpoint_2.pt ..\\trained_models\\GNN_baseline_3_modif_debiased\\GNN3_split_two_without_node_embedding\\history_training\\checkpoint_2.pt\n",
      "Loading best checkpoint number 1 of the epoch 2050.0 with a loss of 0.5107812829972882\n",
      "Loading best checkpoint number 1 of the epoch 2550.0 with a loss of 0.1984780294417413\n",
      "Loading best checkpoint number 2 of the epoch 2700.0 with a loss of 10.3904066518488\n",
      "..\\trained_models\\GNN_baseline_3_modif\\GNN1_baseline\\history_training\\checkpoint_1.pt ..\\trained_models\\GNN_baseline_3_modif\\GNN2_baseline\\history_training\\checkpoint_1.pt ..\\trained_models\\GNN_baseline_3_modif\\GNN3_split_two_without_node_embedding\\history_training\\checkpoint_2.pt\n"
     ]
    }
   ],
   "source": [
    "GNNs_q = Model_GNNs(exp)\n",
    "GNNs_a = Model_GNNs(exp2)\n",
    "GNNs_pi = Model_GNNs(exp)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "Module_Gen = Sampling_Path_Batch(GNNs_q, GNNs_a, GNNs_pi, features = None, lambdas= None, device = device, batch_size = 5, args=exp)\n",
    "\n",
    "Module_Gen.full_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\autograd\\__init__.py:197: UserWarning: Error detected in SoftmaxBackward0. Traceback of forward call that caused the error:\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 725, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\asyncio\\base_events.py\", line 1906, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 513, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 502, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 409, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\goupi\\AppData\\Local\\Temp\\ipykernel_3372\\860078347.py\", line 8, in <module>\n",
      "    Module_Gen.full_generation()\n",
      "  File \"C:\\Users\\goupi\\AppData\\Local\\Temp\\ipykernel_3372\\2606760429.py\", line 264, in full_generation\n",
      "    self.one_step()\n",
      "  File \"C:\\Users\\goupi\\AppData\\Local\\Temp\\ipykernel_3372\\2606760429.py\", line 218, in one_step\n",
      "    pi_softmax_input = compute_softmax_GNN3(pi_choice_input, self.batch_mol_graph.batch, mask)\n",
      "  File \"C:\\Users\\goupi\\AppData\\Local\\Temp\\ipykernel_3372\\138157885.py\", line 102, in compute_softmax_GNN3\n",
      "    choice_softmax = F.softmax(choice_masked, dim=1)\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\nn\\functional.py\", line 1841, in softmax\n",
      "    ret = input.softmax(dim)\n",
      "  File \"c:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\fx\\traceback.py\", line 57, in format_stack\n",
      "    return traceback.format_stack()\n",
      " (Triggered internally at ..\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:119.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [5, 116]], which is output 0 of SoftmaxBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m loss \u001b[39m=\u001b[39m Module_Gen\u001b[39m.\u001b[39mpi_value[\u001b[39m0\u001b[39m]\n\u001b[0;32m      2\u001b[0m loss\u001b[39m.\u001b[39mretain_grad()\n\u001b[1;32m----> 3\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [5, 116]], which is output 0 of SoftmaxBackward0, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "loss = Module_Gen.pi_value[0]\n",
    "loss.retain_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4301e-21, 1.2601e-12, 3.6049e-12, 2.0494e-11, 4.4658e-14],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Module_Gen.q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1649e-29, 2.4139e-12, 1.7741e-12, 4.4168e-13, 1.4290e-15],\n",
       "       device='cuda:0', dtype=torch.float64, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Module_Gen.a_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0808e-12, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Average q_value\n",
    "print(torch.mean(Module_Gen.q_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2638e-14, device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Compute the geometric mean of the q_values but be careful because the number are very small so we use the log trick\n",
    "\n",
    "print(torch.exp(torch.mean(torch.log(Module_Gen.q_value))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.3576e-14])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(torch.tensor([-30.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
