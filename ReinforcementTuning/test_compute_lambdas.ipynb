{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from DataPipeline.preprocessing import node_encoder, tensor_to_smiles\n",
    "from generation import Sampling_Path_Batch\n",
    "from models import Model_GNNs\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Experiment:\n",
    "    exp_name: str\n",
    "    encod: str\n",
    "    keku: bool\n",
    "    train: bool\n",
    "    encoding_size: int = 13\n",
    "    edge_size: int = 3\n",
    "    encoding_option: str = 'charged'\n",
    "    compute_lambdas: bool = False\n",
    "\n",
    "exp = Experiment('GNN_baseline_3_modif', 'charged', True, False, 13, 3, 'charged', True)\n",
    "exp2 = Experiment('GNN_baseline_3_modif_debiased', 'charged', True, False, 13, 3, 'charged', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "def logP(smiles_list):\n",
    "    logP_values = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            logP_values.append(0)\n",
    "        else: \n",
    "            logp = Descriptors.MolLogP(mol)\n",
    "            if logp > 2.0 and logp < 2.5:\n",
    "                logP_values.append(1)\n",
    "            else:\n",
    "                logP_values.append(0)\n",
    "    return torch.tensor(logP_values)\n",
    "\n",
    "def QED(smiles_list):\n",
    "    qed_values = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            qed_values.append(0)\n",
    "        else: \n",
    "            qed = Descriptors.qed(mol)\n",
    "            if qed > 0.90:\n",
    "                qed_values.append(1)\n",
    "            else:\n",
    "                qed_values.append(0)\n",
    "    return torch.tensor(qed_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint number 1 of the epoch 2050.0 with a loss of 0.5107812829972882\n",
      "Loading best checkpoint number 1 of the epoch 2550.0 with a loss of 0.1984780294417413\n",
      "Loading best checkpoint number 2 of the epoch 2700.0 with a loss of 10.3904066518488\n",
      "..\\trained_models\\GNN_baseline_3_modif\\GNN1_baseline\\history_training\\checkpoint_1.pt ..\\trained_models\\GNN_baseline_3_modif\\GNN2_baseline\\history_training\\checkpoint_1.pt ..\\trained_models\\GNN_baseline_3_modif\\GNN3_split_two_without_node_embedding\\history_training\\checkpoint_2.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint number 1 of the epoch 2950.0 with a loss of 0.5103764619447081\n",
      "Loading best checkpoint number 2 of the epoch 2150.0 with a loss of 0.19804639930989767\n",
      "Loading best checkpoint number 2 of the epoch 2700.0 with a loss of 10.3904066518488\n",
      "..\\trained_models\\GNN_baseline_3_modif_debiased\\GNN1_charged_baseline_debiased\\history_training\\checkpoint_1.pt ..\\trained_models\\GNN_baseline_3_modif_debiased\\GNN2_charged_baseline_debiased\\history_training\\checkpoint_2.pt ..\\trained_models\\GNN_baseline_3_modif_debiased\\GNN3_split_two_without_node_embedding\\history_training\\checkpoint_2.pt\n",
      "Loading best checkpoint number 1 of the epoch 2950.0 with a loss of 0.5103764619447081\n",
      "Loading best checkpoint number 2 of the epoch 2150.0 with a loss of 0.19804639930989767\n",
      "Loading best checkpoint number 2 of the epoch 2700.0 with a loss of 10.3904066518488\n",
      "..\\trained_models\\GNN_baseline_3_modif_debiased\\GNN1_charged_baseline_debiased\\history_training\\checkpoint_1.pt ..\\trained_models\\GNN_baseline_3_modif_debiased\\GNN2_charged_baseline_debiased\\history_training\\checkpoint_2.pt ..\\trained_models\\GNN_baseline_3_modif_debiased\\GNN3_split_two_without_node_embedding\\history_training\\checkpoint_2.pt\n"
     ]
    }
   ],
   "source": [
    "GNNs_q = Model_GNNs(exp)\n",
    "GNNs_a = Model_GNNs(exp2)\n",
    "GNNs_pi = Model_GNNs(exp2)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "Module_Gen = Sampling_Path_Batch(GNNs_q, GNNs_a, GNNs_pi, features = {'logP' : logP, 'QED' : QED}, lambdas = torch.Tensor([1.0, 1.0]), device = device, args=exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_optimal_lambdas(Module_Gen, desired_moments, sample_size=5, n_iters=1000, lr=.5, min_nabla_lambda = 0.001, batch_size = 1000): #how do they define the learning rate and sample size maybe do more\n",
    "        \"\"\"\n",
    "        This performs the first step: Constraints --> EBM through self-normalized importance sampling. \n",
    "        Args:\n",
    "            sample_size: total number of samples to use for lambda computation\n",
    "        Returns:\n",
    "            dicitonary of optimal lambdas per constraint: {'black': lambda_1, 'positive': lambda_2}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        print(\"Computing Optimal Lambdas for desired moments...\")\n",
    "\n",
    "        max_n_iters = n_iters\n",
    "\n",
    "        feature_names = list(Module_Gen.features.keys())\n",
    "        mu_star = desired_moments #name mu_bar in the pseudo code\n",
    "\n",
    "        mu_star = torch.tensor([mu_star[f] for f in feature_names])\n",
    "        lambdas = Module_Gen.lambdas.cpu()\n",
    "\n",
    "        # Collect sample_size samples for this:\n",
    "        list_feature_tensor = []\n",
    "        for i in tqdm(range(sample_size)):\n",
    "            #if we do multi processing, i think here the best \n",
    "            #put pi without grad for lambdas\n",
    "\n",
    "            Module_Gen.full_generation(batch_size = batch_size)\n",
    "            Module_Gen.convert_to_smiles()\n",
    "            Module_Gen.compute_features()\n",
    "\n",
    "            q_value = Module_Gen.q_value\n",
    "            a_value = Module_Gen.a_value\n",
    "            pi_value = Module_Gen.pi_value\n",
    "\n",
    "            batch_features_values = Module_Gen.all_features_values\n",
    "\n",
    "            Module_Gen.clean_memory()\n",
    "            \n",
    "            list_feature_tensor.append(batch_features_values)\n",
    "\n",
    "        all_feature_tensor = torch.cat(list_feature_tensor, dim=0)  # [sample_sz*size_batch x F]\n",
    "\n",
    "        print(\"mean logP : \", all_feature_tensor[:, 0].mean())\n",
    "        print(\"mean QED : \", all_feature_tensor[:, 1].mean())\n",
    "\n",
    "        #### check for zero-occuring features. \n",
    "        # If a constraint has not occurred in your sample, no lambdas will be learned for that constraint, so we must check.\n",
    "\n",
    "        for i, feature  in enumerate(feature_names):\n",
    "            assert all_feature_tensor[:, i].sum().item() > 0, \"constraint {feature} hasn't occurred in the samples, use a larger sample size\"\n",
    "\n",
    "        for step in range(max_n_iters): #SGD for finding lambdas\n",
    "\n",
    "            # 1. calculate P_over_q batch wise with current lambdas which will be name w\n",
    "            ## compute new exponents\n",
    "\n",
    "            w = torch.exp(torch.matmul(all_feature_tensor, lambdas.to(all_feature_tensor.device)))\n",
    "            print(w.shape)\n",
    "            print(w)\n",
    "            print(all_feature_tensor.shape)\n",
    "            print(all_feature_tensor)\n",
    "            print(lambdas.shape)\n",
    "            print(lambdas)\n",
    "\n",
    "            # 2. compute mu (mean) of features given the current lambda using SNIS\n",
    "            mu_lambda_numerator = w.view(1, -1).matmul(all_feature_tensor).squeeze(0) # F\n",
    "            print(mu_lambda_numerator.shape)\n",
    "            print(mu_lambda_numerator)\n",
    "            mu_lambda_denominator = w.sum()\n",
    "            mu_lambda = mu_lambda_numerator / mu_lambda_denominator # F\n",
    "\n",
    "            # 3. Update current Lambdas\n",
    "            nabla_lambda = mu_star - mu_lambda.cpu()\n",
    "            err = np.linalg.norm(nabla_lambda.cpu().numpy())\n",
    "            print(\"step: %s \\t ||nabla_lambda|| = %.6f\" %(step, err))\n",
    "            lambdas = lambdas + lr * nabla_lambda\n",
    "            print(\"\\tlambdas : {} \".format(Module_Gen.lambdas))\n",
    "            print(\"\\tμ: {}\".format(mu_lambda))\n",
    "            print(\"\\tμ*: {}\".format(mu_star))\n",
    "\n",
    "            Module_Gen.lambdas = lambdas\n",
    "            \n",
    "            ## Check if error is less than tolerance, then break.\n",
    "            if err < min_nabla_lambda: \n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Optimal Lambdas for desired moments...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:54:26] Explicit valence for atom # 17 O, 3, is greater than permitted\n",
      "[16:54:27] Explicit valence for atom # 17 O, 3, is greater than permitted\n",
      " 40%|████      | 2/5 [02:07<03:14, 64.70s/it][16:56:44] Explicit valence for atom # 18 O, 3, is greater than permitted\n",
      "[16:56:45] Explicit valence for atom # 18 O, 3, is greater than permitted\n",
      " 80%|████████  | 4/5 [04:25<01:07, 67.40s/it]"
     ]
    }
   ],
   "source": [
    "compute_optimal_lambdas(Module_Gen, {'logP': 1.0, 'QED': 1.0}, sample_size=5, n_iters=10000, lr=.2, min_nabla_lambda = 0.0001, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (794465100.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    tensor([7.6847, 8.2535])  5000\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tensor([7.6847, 8.2535])  5000\n",
    "tensor([7.6169, 8.3329])   6000\n",
    "lambdas : tensor([7.8733, 8.4059])  1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
