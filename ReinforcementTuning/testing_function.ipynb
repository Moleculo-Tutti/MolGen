{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "sys.path.append(parent_dir)\n",
    "from DataPipeline.preprocessing import node_encoder\n",
    "from models import Model_GNNs\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_torch_graph_from_one_atom(atom, edge_size, encoding_option='charged'):\n",
    "    num_atom = int(atom)\n",
    "\n",
    "    atom_attribute = node_encoder(num_atom, encoding_option=encoding_option)\n",
    "    # Create graph\n",
    "    graph = torch_geometric.data.Data(x=atom_attribute.view(1, -1), edge_index=torch.empty((2, 0), dtype=torch.long), edge_attr=torch.empty((0, edge_size)))\n",
    "\n",
    "    return graph\n",
    "\n",
    "def sample_first_atom(encoding = 'charged'):\n",
    "    if encoding == 'reduced' or encoding == 'charged':\n",
    "        prob_dict = {'60': 0.7385023585929047, \n",
    "                    '80': 0.1000143018658728, \n",
    "                    '70': 0.12239949901813525, \n",
    "                    '90': 0.013786373862576426, \n",
    "                    '160': 0.017856330814654413,\n",
    "                    '170': 0.007441135845856433}\n",
    "    if encoding == 'polymer':\n",
    "        prob_dict = {'60': 0.7489344573582472,\n",
    "                    '70': 0.0561389266682314,\n",
    "                    '80': 0.0678638375933265,\n",
    "                    '160': 0.08724385192820308,\n",
    "                    '90': 0.032130486119902095,\n",
    "                    '140': 0.007666591133009364,\n",
    "                    '150': 2.184919908044154e-05}\n",
    "\n",
    "    \n",
    "    return random.choices(list(prob_dict.keys()), weights=list(prob_dict.values()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "edge_size = 3\n",
    "true_graphs = [create_torch_graph_from_one_atom(sample_first_atom(), edge_size=edge_size, encoding_option='charged')\n",
    "                            for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([], size=(2, 0), dtype=torch.int64)\n",
      "tensor([], size=(0, 3))\n"
     ]
    }
   ],
   "source": [
    "print(true_graphs[0].x.shape)\n",
    "print(true_graphs[0].x)\n",
    "print(true_graphs[0].edge_index)\n",
    "print(true_graphs[0].edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_value = torch.zeros(batch_size)\n",
    "q_value = torch.zeros(batch_size)\n",
    "pi_value = torch.zeros(batch_size)\n",
    "queues = [[0] for i in range(batch_size)]\n",
    "\n",
    "#not_graphs_finished is a mask of size batch size, initialized at true\n",
    "not_mols_finished = [True for i in range(batch_size)]\n",
    "\n",
    "#while not all atoms are finished\n",
    "current_atoms = torch.zeros(batch_size)\n",
    "for i in range(batch_size):\n",
    "    if len(queues[i]) == 0:\n",
    "        not_mols_finished[i] = False\n",
    "        current_atoms[i]= -1 # precise no current_atoms\n",
    "    else :\n",
    "        current_atoms[i] = queues[i][0]\n",
    "\n",
    "graph_for_gnn1 =[graph.clone() for graph in true_graphs]\n",
    "#add feature position\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1.x = torch.cat([graph1.x, torch.zeros(graph1.x.size(0), 1)], dim=1)\n",
    "for graph1 in graph_for_gnn1 :\n",
    "        graph1.x[0:current_atoms[i], -1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'e', 'f']\n"
     ]
    }
   ],
   "source": [
    "liste = ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "indices_a_supprimer = [0, 0, 0]\n",
    "\n",
    "# Tri de la liste d'indices dans l'ordre inverse\n",
    "indices_a_supprimer.sort(reverse=True)\n",
    "\n",
    "# Suppression des éléments\n",
    "for indice in indices_a_supprimer:\n",
    "    liste.pop(indice)\n",
    "\n",
    "print(liste) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d', 'e', 'f']\n"
     ]
    }
   ],
   "source": [
    "liste = ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "indices_a_supprimer = [0, 1, 2]\n",
    "\n",
    "# Tri de la liste d'indices dans l'ordre inverse\n",
    "indices_a_supprimer.sort(reverse=True)\n",
    "\n",
    "# Suppression des éléments\n",
    "for indice in indices_a_supprimer:\n",
    "    liste.pop(indice)\n",
    "\n",
    "print(liste) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "liste = [0,1,2,3,4,5]\n",
    "liste.pop(0)\n",
    "print(liste)\n",
    "print(liste[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#compute the gnn1 and apply softmax\u001b[39;00m\n\u001b[0;32m      2\u001b[0m batch_for_gnn1 \u001b[39m=\u001b[39m Batch\u001b[39m.\u001b[39mfrom_data_list(graph_for_gnn1)\n\u001b[1;32m----> 3\u001b[0m batch_for_gnn1\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m output_gnn1_q \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(model_q\u001b[39m.\u001b[39mGNN1(batch_for_gnn1), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m output_gnn1_a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(model_a\u001b[39m.\u001b[39mGNN1(batch_for_gnn1),  dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#compute the gnn1 and apply softmax\n",
    "batch_for_gnn1 = Batch.from_data_list(graph_for_gnn1)\n",
    "batch_for_gnn1.to(device)\n",
    "output_gnn1_q = torch.softmax(model_q.GNN1(batch_for_gnn1), dim = 1)\n",
    "output_gnn1_a = torch.softmax(model_a.GNN1(batch_for_gnn1),  dim = 1)\n",
    "output_gnn1_pi = torch.softmax(model_pi.GNN1(batch_for_gnn1), dim = 1)\n",
    "\n",
    "#for each graph in the ouptut_gnn1_q we sample with a multinomial\n",
    "next_step_gnn1 = torch.multinomial(output_gnn1_q, 1).item() #shape (batch_size, 1)\n",
    "\n",
    "a_value = a_value * torch([output_gnn1_a[i][next_step_gnn1[i]] for i in range(batch_size) if not_mols_finished[i]])\n",
    "pi_value = pi_value * torch([output_gnn1_pi[i][next_step_gnn1[i]] for i in range(batch_size) if not_mols_finished[i]])\n",
    "q_value = q_value * torch([output_gnn1_q[i][next_step_gnn1[i]] for i in range(batch_size) if not_mols_finished[i]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
