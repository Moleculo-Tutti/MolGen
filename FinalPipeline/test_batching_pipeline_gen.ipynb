{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import concurrent\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from DataPipeline.preprocessing import process_encode_graph, get_subgraph_with_terminal_nodes_step\n",
    "from DataPipeline.preprocessing import node_encoder\n",
    "from Model.GNN1 import ModelWithEdgeFeatures as GNN1\n",
    "from Model.GNN1 import ModelWithNodeConcat as GNN1_node_concat\n",
    "from Model.GNN2 import ModelWithEdgeFeatures as GNN2\n",
    "from Model.GNN2 import ModelWithNodeConcat as GNN2_node_concat\n",
    "from Model.GNN3 import ModelWithEdgeFeatures as GNN3\n",
    "from Model.GNN3 import ModelWithgraph_embedding_modif as GNN3_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_smiles(node_features, edge_index, edge_attr, edge_mapping = 'aromatic', encoding_type = 'charged'):\n",
    "    # Create an empty editable molecule\n",
    "    mol = Chem.RWMol()\n",
    "\n",
    "    # Define atom mapping\n",
    "    if encoding_type == 'charged':\n",
    "        \n",
    "        atom_mapping = {\n",
    "            0: ('C', 0),\n",
    "            1: ('N', 0),\n",
    "            2: ('N', 1),\n",
    "            3: ('N', -1),\n",
    "            4: ('O', 0),\n",
    "            5: ('O', -1),\n",
    "            6: ('F', 0),\n",
    "            7: ('S', 0),\n",
    "            8: ('S', -1),\n",
    "            9: ('Cl', 0),\n",
    "            10: ('Br', 0),\n",
    "            11: ('I', 0)\n",
    "        }\n",
    "\n",
    "    elif encoding_type == 'polymer':\n",
    "        atom_mapping = {\n",
    "            0: ('C', 0),\n",
    "            1: ('N', 0),\n",
    "            2: ('O', 0),\n",
    "            3: ('F', 0),\n",
    "            4: ('Si', 0),\n",
    "            5: ('P', 0),\n",
    "            6: ('S', 0)}\n",
    "\n",
    "    # Add atoms\n",
    "    for atom_feature in node_features:\n",
    "        atom_idx = atom_feature[:12].argmax().item()\n",
    "        atom_symbol, charge = atom_mapping.get(atom_idx)\n",
    "        atom = Chem.Atom(atom_symbol)\n",
    "        atom.SetFormalCharge(charge)\n",
    "        mol.AddAtom(atom)\n",
    "\n",
    "    # Define bond type mapping\n",
    "    if edge_mapping == 'aromatic':\n",
    "        bond_mapping = {\n",
    "            0: Chem.rdchem.BondType.AROMATIC,\n",
    "            1: Chem.rdchem.BondType.SINGLE,\n",
    "            2: Chem.rdchem.BondType.DOUBLE,\n",
    "            3: Chem.rdchem.BondType.TRIPLE,\n",
    "        }\n",
    "    elif edge_mapping == 'kekulized':\n",
    "        bond_mapping = {\n",
    "            0: Chem.rdchem.BondType.SINGLE,\n",
    "            1: Chem.rdchem.BondType.DOUBLE,\n",
    "            2: Chem.rdchem.BondType.TRIPLE,\n",
    "        }\n",
    "\n",
    "    # Add bonds\n",
    "    for start, end, bond_attr in zip(edge_index[0], edge_index[1], edge_attr):\n",
    "        bond_type_idx = bond_attr[:4].argmax().item()\n",
    "        bond_type = bond_mapping.get(bond_type_idx)\n",
    "\n",
    "        # RDKit ignores attempts to add a bond that already exists,\n",
    "        # so we need to check if the bond exists before we add it\n",
    "        if mol.GetBondBetweenAtoms(start.item(), end.item()) is None:\n",
    "            mol.AddBond(start.item(), end.item(), bond_type)\n",
    "\n",
    "    # Convert the molecule to SMILES\n",
    "    smiles = Chem.MolToSmiles(mol)\n",
    "\n",
    "    return smiles\n",
    "\n",
    "def sample_random_subgraph_ZINC(pd_dataframe, start_size):\n",
    "    indice = random.choice(pd_dataframe.index)\n",
    "    smiles_str = pd_dataframe.loc[indice, 'smiles']\n",
    "\n",
    "    torch_graph = process_encode_graph(smiles_str, encoding_option='reduced')\n",
    "    subgraph_data, terminal_node_info, id_map = get_subgraph_with_terminal_nodes_step(torch_graph, start_size)\n",
    "\n",
    "    return subgraph_data, terminal_node_info, id_map\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "def sample_first_atom_batch(batch_size, encoding = 'reduced'):\n",
    "    if encoding == 'reduced' or encoding == 'charged':\n",
    "        prob_dict = {'60': 0.7385023585929047, \n",
    "                    '80': 0.1000143018658728, \n",
    "                    '70': 0.12239949901813525, \n",
    "                    '90': 0.013786373862576426, \n",
    "                    '160': 0.017856330814654413,\n",
    "                    '170': 0.007441135845856433}\n",
    "    if encoding == 'polymer':\n",
    "        prob_dict = {'60': 0.7489344573582472,\n",
    "                    '70': 0.0561389266682314,\n",
    "                    '80': 0.0678638375933265,\n",
    "                    '160': 0.08724385192820308,\n",
    "                    '90': 0.032130486119902095,\n",
    "                    '140': 0.007666591133009364,\n",
    "                    '150': 2.184919908044154e-05}\n",
    "\n",
    "    atoms = [random.choices(list(prob_dict.keys()), weights=list(prob_dict.values()))[0] for _ in range(batch_size)]\n",
    "    return atoms\n",
    "\n",
    "def create_torch_graph_from_one_atom_list(atoms, edge_size, encoding_option='reduced') -> list:\n",
    "    graphs = []\n",
    "    for atom in atoms:\n",
    "        num_atom = int(atom)\n",
    "        atom_attribute = node_encoder(num_atom, encoding_option=encoding_option)\n",
    "        # Create graph\n",
    "        graph = torch_geometric.data.Data(x=atom_attribute.view(1, -1), edge_index=torch.empty((2, 0), dtype=torch.long), edge_attr=torch.empty((0, edge_size)))\n",
    "        graphs.append(graph)\n",
    "\n",
    "    \n",
    "    return graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = sample_first_atom_batch(1000, encoding = 'charged')\n",
    "batch = create_torch_graph_from_one_atom_batch(atoms, 3, encoding_option='charged')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[1000, 13], edge_index=[2, 0], edge_attr=[0, 3], batch=[1000], ptr=[1001])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_current_nodes_batched(batch_graph, current_nodes_tensor : torch.tensor):\n",
    "\n",
    "    # Count the number of nodes in each graph up to (but not including) the current node\n",
    "    node_counts = batch_graph.batch.bincount().cumsum(0)\n",
    "    offsets = torch.roll(node_counts, shifts=1, dims=0)\n",
    "    offsets[0] = 0  # The first graph has no offset\n",
    "\n",
    "    # Add the offsets to the current_nodes indices\n",
    "    current_nodes_tensor += offsets\n",
    "\n",
    "    return current_nodes_tensor\n",
    "        \n",
    "\n",
    "def set_feature_position(batch_graph, current_nodes_tensor):\n",
    "    \n",
    "    # Add feature position\n",
    "    batch_graph.x = torch.cat([batch_graph.x, torch.zeros(batch_graph.x.size(0), 1)], dim=1)\n",
    "    current_nodes_expanded = current_nodes_tensor[batch_graph.batch]\n",
    "    # Create a tensor that contains the cumulative sum of nodes up to the current node in each graph\n",
    "    cumulative_current_nodes = (torch.arange(batch_graph.x.size(0)) > (current_nodes_expanded)).float()\n",
    "    # Flatten cumulative_current_nodes before using it for indexing\n",
    "    cumulative_current_nodes = cumulative_current_nodes.view(-1)\n",
    "    # Set the feature position for each node\n",
    "    batch_graph.x[:, -1] = cumulative_current_nodes[batch_graph.batch]\n",
    "\n",
    "    return batch_graph\n",
    "\n",
    "def set_last_nodes(batch_graph):\n",
    "    \n",
    "    node_counts = batch_graph.batch.bincount().cumsum(0)\n",
    "    offsets = torch.roll(node_counts, shifts=1, dims=0)\n",
    "    offsets[0] = 0  # The first graph has no offset\n",
    "    last_nodes_each_graph = batch_graph.batch.bincount() - 1 + offsets\n",
    "    batch_graph.x[last_nodes_each_graph, -1] = 1\n",
    "\n",
    "    return batch_graph, last_nodes_each_graph\n",
    "\n",
    "\n",
    "def create_mask(batch_graph, current_nodes_tensor : torch.tensor, last_nodes):\n",
    "    total_nodes = batch_graph.x.size(0)\n",
    "    # Expand current_nodes to match the total number of nodes\n",
    "    expanded_current_nodes = current_nodes_tensor[batch_graph.batch]\n",
    "    mask = torch.arange(total_nodes) > expanded_current_nodes\n",
    "    # Create a new mask where the last node of each graph is marked as False\n",
    "    mask[last_nodes] = False\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n",
    "def add_nodes_and_edges(graphs, new_nodes, new_edges, current_nodes, mask_gnn2):\n",
    "    # Iterate over the graphs, new nodes and edges, current nodes and mask simultaneously\n",
    "    for graph, new_node, new_edge, current_node, add in zip(graphs, new_nodes, new_edges, current_nodes, mask_gnn2):\n",
    "        # If the mask for this graph is False, do nothing\n",
    "        if not add:\n",
    "            continue\n",
    "        \n",
    "        # Add the new node to the graph\n",
    "        graph.x = torch.cat([graph.x, new_node.unsqueeze(0)], dim=0)\n",
    "        \n",
    "        # Add an edge from the new node to the current node, and from the current node to the new node\n",
    "        # to ensure that the graph remains undirected\n",
    "        new_node_index = graph.x.size(0) - 1  # The index of the new node\n",
    "        new_edge_indices = torch.tensor([[new_node_index, current_node], [current_node, new_node_index]], device=graph.edge_index.device)\n",
    "        graph.edge_index = torch.cat([graph.edge_index, new_edge_indices], dim=1)\n",
    "        \n",
    "        # Add the new edge attributes to the graph\n",
    "        graph.edge_attr = torch.cat([graph.edge_attr, new_edge.unsqueeze(0).repeat(2, 1)], dim=0)\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "\n",
    "\n",
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, model, optimizer):\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return model, optimizer\n",
    "\n",
    "def get_model_GNN1(config, encoding_size, edge_size):\n",
    "\n",
    "    return GNN1(in_channels=encoding_size + int(config['feature_position'] + int(len(config['score_list']))),\n",
    "                hidden_channels_list=config[\"GCN_size\"],\n",
    "                mlp_hidden_channels=config['mlp_hidden'],\n",
    "                edge_channels=edge_size, \n",
    "                num_classes=encoding_size, \n",
    "                use_dropout=config['use_dropout'],\n",
    "                size_info=config['use_size'],\n",
    "                max_size=config['max_size'])\n",
    "\n",
    "def get_model_GNN2(config, encoding_size, edge_size):\n",
    "\n",
    "    return GNN2(in_channels=encoding_size + int(config['feature_position'] + int(len(config['score_list']))),\n",
    "                hidden_channels_list=config[\"GCN_size\"],\n",
    "                mlp_hidden_channels=config['mlp_hidden'],\n",
    "                edge_channels=edge_size, \n",
    "                num_classes=edge_size, \n",
    "                size_info=config['use_size'],\n",
    "                max_size=config['max_size'],\n",
    "                use_dropout=config['use_dropout'])\n",
    "\n",
    "def get_model_GNN3(config, encoding_size, edge_size):\n",
    "\n",
    "    if config['graph_embedding']:\n",
    "        return GNN3_embedding(in_channels=encoding_size + int(config['feature_position'] + int(len(config['score_list']))),\n",
    "                    hidden_channels_list=config[\"GCN_size\"],\n",
    "                    mlp_hidden_channels = config['mlp_hidden'],\n",
    "                    edge_channels=edge_size, \n",
    "                    num_classes=edge_size,\n",
    "                    use_dropout=config['use_dropout'],\n",
    "                    size_info=config['use_size'],\n",
    "                    max_size=config['max_size'])\n",
    "\n",
    "    return GNN3(in_channels=encoding_size + int(config['feature_position'] + int(len(config['score_list']))),\n",
    "                hidden_channels_list=config[\"GCN_size\"],\n",
    "                edge_channels=edge_size, \n",
    "                use_dropout=config['use_dropout'])\n",
    "\n",
    "def get_optimizer(model, lr):\n",
    "    return torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "def add_edge_or_node_to_graph(graph, initial_node, edge_attr, other_node=None, new_node_attr=None):\n",
    "    # Convert nodes to tensor\n",
    "    initial_node = torch.tensor([initial_node], dtype=torch.long)\n",
    "\n",
    "    # Create new edge attribute\n",
    "\n",
    "    new_edge_attr = torch.cat([edge_attr, edge_attr], dim=0)\n",
    "\n",
    "    # If new_node_attr is provided, add a new node\n",
    "    if new_node_attr is not None:\n",
    "\n",
    "        # Add new node to graph\n",
    "        assert graph.x.size(1) == new_node_attr.size(1)\n",
    "\n",
    "        graph.x = torch.cat([graph.x, new_node_attr], dim=0)\n",
    "\n",
    "        # Adjust other_node to be the new node\n",
    "        other_node = torch.tensor([graph.x.size(0)-1], dtype=torch.long)\n",
    "    elif other_node is None:\n",
    "        raise ValueError(\"other_node must be provided if no new node is being added\")\n",
    "    else:\n",
    "        other_node = torch.tensor([other_node], dtype=torch.long)\n",
    "\n",
    "    # Create new edge\n",
    "    new_edge = torch.tensor([[initial_node.item(), other_node.item()], \n",
    "                             [other_node.item(), initial_node.item()]], dtype=torch.long)\n",
    "    # Add new edge and its attribute to graph\n",
    "    graph.edge_index = torch.cat([graph.edge_index, new_edge], dim=1)\n",
    "    graph.edge_attr = torch.cat([graph.edge_attr, new_edge_attr], dim=0)\n",
    "\n",
    "    graph.batch = torch.zeros(graph.x.size(0), dtype=torch.long)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def select_node(tensor, edge_size):\n",
    "\n",
    "    # Somme sur les 3 premiÃ¨res dimensions de chaque vecteur\n",
    "    sum_on_first_dims = tensor[:, :edge_size - 1].sum(dim=1)\n",
    "\n",
    "    # Trouver l'indice du node avec la plus grande somme\n",
    "    max_index = torch.argmax(sum_on_first_dims)\n",
    "\n",
    "    return tensor[max_index], max_index\n",
    "\n",
    "def add_score_features(subgraph, scores_list, desired_scores_list, GNN_type = 1):\n",
    "\n",
    "    if scores_list != []:\n",
    "        assert len(scores_list) == len(desired_scores_list) \n",
    "        # Concat the scores to the node features\n",
    "        for i, score in enumerate(scores_list):\n",
    "            score_tensor = torch.tensor(desired_scores_list[i], dtype=torch.float).view(1, 1)\n",
    "            # Duplicate the score tensor to match the number of nodes in the subgraph\n",
    "            score_tensor = score_tensor.repeat(subgraph.x.size(0), 1)\n",
    "            subgraph.x = torch.cat([subgraph.x, score_tensor], dim=-1)\n",
    "        if GNN_type == 2:\n",
    "            subgraph.neighbor = torch.cat([subgraph.neighbor, torch.zeros((1, len(scores_list)))], dim=-1)\n",
    "    return subgraph\n",
    "\n",
    "\n",
    "class MolGen():\n",
    "    def __init__(self, GNN1, GNN2, GNN3, encoding_size, edge_size, batch_size, feature_position, device, save_intermidiate_states = False, encoding_option = 'charged', score_list = [], desired_score_list = []):\n",
    "\n",
    "        self.mol_graphs_list = create_torch_graph_from_one_atom_list(sample_first_atom_batch(batch_size = batch_size, encoding = encoding_option), edge_size=edge_size, encoding_option=encoding_option)\n",
    "        self.queues = [[0] for _ in range(batch_size)]\n",
    "        self.batch_size = batch_size\n",
    "        self.GNN1 = GNN1\n",
    "        self.GNN2 = GNN2\n",
    "        self.GNN3 = GNN3\n",
    "        self.device = device\n",
    "        self.feature_position = feature_position\n",
    "        self.encoding_size = encoding_size\n",
    "        self.edge_size = edge_size\n",
    "        self.save_intermidiate_states = save_intermidiate_states\n",
    "        if save_intermidiate_states:\n",
    "            self.intermidiate_states = []\n",
    "\n",
    "        self.score_list = score_list\n",
    "        self.desired_score_list = desired_score_list\n",
    "        self.finished_molecules = []\n",
    "\n",
    "    def one_step(self):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Lists to store the updated queues and current_nodes\n",
    "            updated_queues = []\n",
    "            current_nodes = []\n",
    "            new_mol_graphs_list = []\n",
    "\n",
    "            # Loop over each queue\n",
    "            for i, queue in enumerate(self.queues):\n",
    "                # If a queue is empty, its corresponding molecule is finished\n",
    "                if not queue:\n",
    "                    self.finished_molecules.append(self.mol_graphs_list[i])\n",
    "                else:\n",
    "                    updated_queues.append(queue)\n",
    "                    new_mol_graphs_list.append(self.mol_graphs_list[i])\n",
    "                    current_nodes.append(queue[0])\n",
    "\n",
    "            # Replace the original mol_graphs_list with the new one\n",
    "            self.mol_graphs_list = new_mol_graphs_list\n",
    "            self.queues = updated_queues\n",
    "\n",
    "            \"\"\"\n",
    "            Prepare the GNN 1 BATCH\n",
    "            \"\"\"\n",
    "            batch_graph12 = Batch.from_data_list(self.mol_graphs_list).to(self.device)\n",
    "            current_nodes_tensor = torch.tensor(current_nodes, dtype=torch.long)\n",
    "            current_nodes_batched = return_current_nodes_batched(batch_graph12, current_nodes_tensor) #reindex the nodes to match the batch size\n",
    "            batch_graph12.x[current_nodes_batched, -1] = 1\n",
    "            \n",
    "            if self.feature_position:\n",
    "                batch_graph12 = set_feature_position(batch_graph12, current_nodes_batched)\n",
    "            \n",
    "            batch_graph12 = add_score_features(batch_graph12, self.score_list, self.desired_score_list, GNN_type = 1)\n",
    "\n",
    "            predictions = self.GNN1(batch_graph12.to(self.device))\n",
    "\n",
    "            # Apply softmax to prediction\n",
    "            softmax_predictions = F.softmax(predictions, dim=1)\n",
    "            # Sample next node from prediction\n",
    "            predicted_nodes = torch.multinomial(softmax_predictions, num_samples=1)\n",
    "\n",
    "            # Create a mask for determining which graphs should continue to GNN2\n",
    "            mask_gnn2 = (predicted_nodes != self.encoding_size - 1).squeeze()\n",
    "\n",
    "            # Handle the stopping condition (where predicted_node is encoding_size - 1)\n",
    "            stop_mask = (predicted_nodes == self.encoding_size - 1).squeeze()\n",
    "\n",
    "            # Remove the first node from the queues of graphs that have stopped\n",
    "            self.queues = [queue[1:] for queue, stopped in zip(self.queues, stop_mask) if not stopped]\n",
    "\n",
    "            # Encode next node for the entire batch\n",
    "            encoded_predicted_nodes = torch.zeros(predictions.size(), device=self.device, dtype=torch.float)\n",
    "            encoded_predicted_nodes.scatter_(1, predicted_nodes, 1)\n",
    "\n",
    "            # Add the size of x to the queue for graphs that haven't stopped\n",
    "            self.queues = [queue + [graph.x.size(0)] if continue_gnn2 else queue\n",
    "                        for queue, graph, continue_gnn2 in zip(self.queues, self.mol_graphs_list, mask_gnn2)]\n",
    "                \n",
    "\n",
    "            #GNN2 \n",
    "\n",
    "            if self.feature_position:\n",
    "                    \n",
    "                # add zeros to the neighbor\n",
    "                encoded_predicted_nodes = torch.cat([encoded_predicted_nodes, torch.zeros(self.batch_size, 1).to(encoded_predicted_nodes.device)], dim=1)\n",
    "\n",
    "            batch_graph12.neighbor = encoded_predicted_nodes\n",
    "\n",
    "            batch_graph12 = add_score_features(batch_graph12, self.score_list, self.desired_score_list, GNN_type = 2)\n",
    "\n",
    "            assert batch_graph12.x.size(1) == batch_graph12.neighbor.size(1)\n",
    "            \n",
    "            predictions2 = self.GNN2(batch_graph12.to(self.device))\n",
    "\n",
    "            predicted_edges = torch.multinomial(F.softmax(predictions2, dim=1), num_samples=1)\n",
    "            encoded_predicted_edges = torch.zeros(predictions2.size(), device=self.device, dtype=torch.float)\n",
    "            encoded_predicted_edges.scatter_(1, predicted_edges, 1)\n",
    "            \n",
    "            # Create a new node that is going to be added to the graph for each batch\n",
    "            new_nodes = torch.zeros(self.batch_size, self.encoding_size, device=self.device, dtype=torch.float)\n",
    "            new_nodes.scatter_(1, predicted_nodes, 1)\n",
    "\n",
    "\n",
    "            #GNN3\n",
    "\n",
    "            # Add the node and the edge to the graph\n",
    "            new_graph_list = add_nodes_and_edges(batch_graph12, new_nodes, encoded_predicted_edges, current_nodes, mask_gnn2)\n",
    "            batch_graph3 = Batch.from_data_list(new_graph_list).to(self.device)\n",
    "\n",
    "            batch_graph3, last_nodes = set_last_nodes(batch_graph3)\n",
    "\n",
    "            current_nodes_batched = return_current_nodes_batched(batch_graph3, current_nodes_tensor) #reindex the nodes to match the batch size\n",
    "            \n",
    "            if self.feature_position:\n",
    "                batch_graph3 = set_feature_position(batch_graph3, current_nodes_batched)\n",
    "            \n",
    "            mask = create_mask(batch_graph3, current_nodes = current_nodes, last_nodes = last_nodes)\n",
    "\n",
    "            batch_graph3.mask = mask\n",
    "\n",
    "\n",
    "            batch_graph3 = add_score_features(batch_graph3, self.score_list, self.desired_score_list, GNN_type = 3)\n",
    "\n",
    "            prediction3 = self.GNN3(batch_graph3.to(self.device))\n",
    "            \n",
    "            softmax_prediction3 = F.softmax(prediction3, dim=1)[mask]\n",
    "        \n",
    "        \n",
    "            \"\"\"\n",
    "            if softmax_prediction3.size(0) == 0:\n",
    "                #Stop\n",
    "                self.mol_graph = new_graph_batch\n",
    "                return\n",
    "            \"\"\"\n",
    "            print(softmax_prediction3)\n",
    "            selected_edge_distribution, max_index = select_node(softmax_prediction3, edge_size=self.edge_size)\n",
    "\n",
    "             #sample edge\n",
    "            predicted_edge = torch.multinomial(selected_edge_distribution, 1).item()\n",
    "\n",
    "            if predicted_edge == self.edge_size - 1:\n",
    "                #Stop\n",
    "                self.mol_graph = new_graph\n",
    "                return\n",
    "\n",
    "            encoded_predicted_edge = torch.zeros(prediction2.size(), dtype=torch.float)\n",
    "            encoded_predicted_edge[0, predicted_edge] = 1\n",
    "\n",
    "\n",
    "            output_graph = add_edge_or_node_to_graph(new_graph, graph1.x.size(0), encoded_predicted_edge, other_node=current_node + max_index+1)\n",
    "\n",
    "            self.mol_graph = output_graph\n",
    "\n",
    "    def full_generation(self):\n",
    "        max_iter = 300\n",
    "        i = 0\n",
    "        while len(self.queues) > 0:\n",
    "            if i > max_iter:\n",
    "                break\n",
    "            self.one_step()\n",
    "            i += 1\n",
    "\n",
    "            if self.save_intermidiate_states:\n",
    "                self.intermidiate_states.append(self.mol_graph.clone())\n",
    "        \n",
    "    def is_valid(self):\n",
    "        if self.edge_size == 3:\n",
    "            edge_mapping = 'kekulized'\n",
    "        else:\n",
    "            edge_mapping = 'aromatic'\n",
    "        SMILES_str = tensor_to_smiles(self.mol_graph.x, self.mol_graph.edge_index, self.mol_graph.edge_attr, edge_mapping, encoding_type=self.encoding_type)\n",
    "        mol = Chem.MolFromSmiles(SMILES_str)\n",
    "        if mol is None:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "class GenerationModule():\n",
    "    def __init__(self, config1, config2, config3, encoding_size, edge_size, pathGNN1, pathGNN2, pathGNN3, checking_mode = False, encoding_type = 'charged', batch_size = 64, score_list = [], desired_score_list = []):\n",
    "        self.config1 = config1\n",
    "        self.config2 = config2\n",
    "        self.config3 = config3\n",
    "        self.encoding_size = encoding_size\n",
    "        self.edge_size = edge_size\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_type = encoding_type\n",
    "        self.feature_position = config1[\"feature_position\"]\n",
    "        self.checking_mode = checking_mode\n",
    "\n",
    "        self.score_list = config1[\"score_list\"]\n",
    "        self.desired_score_list = desired_score_list\n",
    "\n",
    "        if self.checking_mode:\n",
    "            self.non_valid_molecules = []\n",
    "\n",
    "        self.GNN1 = get_model_GNN1(config1, encoding_size, edge_size)\n",
    "        self.GNN2 = get_model_GNN2(config2, encoding_size, edge_size)\n",
    "        self.GNN3 = get_model_GNN3(config3, encoding_size, edge_size)\n",
    "\n",
    "        self.optimizer_GNN1 = torch.optim.Adam(self.GNN1.parameters(), lr=config1[\"lr\"])\n",
    "        self.optimizer_GNN2 = torch.optim.Adam(self.GNN2.parameters(), lr=config2[\"lr\"])\n",
    "        self.optimizer_GNN3 = torch.optim.Adam(self.GNN3.parameters(), lr=config3[\"lr\"])\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.GNN1_model, self.optimizer_GNN1 = load_model(pathGNN1, self.GNN1, self.optimizer_GNN1)\n",
    "        self.GNN2_model, self.optimizer_GNN2 = load_model(pathGNN2, self.GNN2, self.optimizer_GNN2)\n",
    "        self.GNN3_model, self.optimizer_GNN3 = load_model(pathGNN3, self.GNN3, self.optimizer_GNN3)\n",
    "\n",
    "        self.GNN1_model.to(self.device)\n",
    "        self.GNN2_model.to(self.device)\n",
    "        self.GNN3_model.to(self.device)\n",
    "\n",
    "        self.GNN1_model.eval()\n",
    "        self.GNN2_model.eval()\n",
    "        self.GNN3_model.eval()\n",
    "    \n",
    "    def generate_single_molecule(self):\n",
    "        mol = MolGen(self.GNN1_model,\n",
    "                     self.GNN2_model,\n",
    "                     self.GNN3_model,\n",
    "                     self.encoding_size,\n",
    "                     self.edge_size,\n",
    "                     self.batch_size,\n",
    "                     self.feature_position,\n",
    "                     self.device,\n",
    "                     save_intermidiate_states=self.checking_mode,\n",
    "                     encoding_option=self.encoding_type,\n",
    "                     score_list=self.score_list,\n",
    "                     desired_score_list=self.desired_score_list)\n",
    "        mol.full_generation()\n",
    "        if self.checking_mode:\n",
    "            # check validity of the molecule\n",
    "            if not mol.is_valid():\n",
    "                self.non_valid_molecules.append(mol.intermidiate_states)\n",
    "        return mol.mol_graph\n",
    "\n",
    "    def generate_mol_list(self, n_mol, n_threads=1):\n",
    "        mol_list = []\n",
    "        if n_threads == 1:\n",
    "            for i in tqdm(range(n_mol), desc=\"Generating molecules\"):\n",
    "                mol_graph = self.generate_single_molecule()\n",
    "                mol_list.append(mol_graph)\n",
    "        else:\n",
    "\n",
    "            # Utilize ThreadPoolExecutor to parallelize the task\n",
    "            with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "                # Submit tasks to the thread pool\n",
    "                future_to_mol = {executor.submit(self.generate_single_molecule): i for i in range(n_mol)}\n",
    "                \n",
    "                # Collect the results as they become available\n",
    "                for future in tqdm(as_completed(future_to_mol), total=n_mol, desc=\"Generating molecules\"):\n",
    "                    mol_graph = future.result()\n",
    "                    mol_list.append(mol_graph)\n",
    "                    \n",
    "        return mol_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All paths \n",
    "\n",
    "GENERATED_SAVING_DIR = Path('..') / 'generated_mols' / 'raw'\n",
    "SCORED_SAVING_DIR = Path('..') / 'generated_mols' / 'scored'\n",
    "ZINC_DATA_PATH = SCORED_SAVING_DIR / 'scored_zinc.csv'\n",
    "\n",
    "SAVE_RESULTS_PATH = Path('..') / 'generate_analyse_mols' / 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_models(path):\n",
    "    with open(path/'six_best_epochs.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        epoch_values = [float(line.split(' ')[1]) for line in lines]\n",
    "        best_line_index = epoch_values.index(max(epoch_values))\n",
    "        loss_value = float(lines[best_line_index].split(' ')[-1])\n",
    "    print('Loading best checkpoint number {} of the epoch {} with a loss of {}'.format(best_line_index, epoch_values[best_line_index], loss_value))\n",
    "    checkpoint_path = path / 'history_training' / f'checkpoint_{best_line_index}.pt'\n",
    "    return checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'GNN_baseline'\n",
    "experiment_path = Path('..') / 'trained_models' / experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best checkpoint number 1 of the epoch 2050.0 with a loss of 0.5107812829972882\n",
      "Loading best checkpoint number 1 of the epoch 2550.0 with a loss of 0.1984780294417413\n",
      "Loading best checkpoint number 0 of the epoch 3000.0 with a loss of 0.05697827504829092\n",
      "..\\trained_models\\GNN_baseline\\GNN1_baseline\\history_training\\checkpoint_1.pt ..\\trained_models\\GNN_baseline\\GNN2_baseline\\history_training\\checkpoint_1.pt ..\\trained_models\\GNN_baseline\\GNN3_baseline\\history_training\\checkpoint_0.pt\n",
      "Generating molecules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating molecules:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating molecules:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_graph3.batch tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])\n",
      "last_nodes_each_graph tensor([1, 3, 5, 7, 9])\n",
      "batch_graph3.x tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "expanded_current_nodes tensor([0, 0, 1, 1, 2, 2, 3, 3, 4, 4])\n",
      "mask tensor([False,  True,  True,  True,  True,  True,  True,  True,  True,  True])\n",
      "mask tensor([False, False,  True, False,  True, False,  True, False,  True, False])\n",
      "tensor([[0.0000e+00, 0.0000e+00, 1.0000e+00],\n",
      "        [8.5399e-23, 0.0000e+00, 1.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00],\n",
      "        [8.5399e-23, 0.0000e+00, 1.0000e+00]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'new_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 55\u001b[0m\n\u001b[0;32m     42\u001b[0m module \u001b[39m=\u001b[39m GenerationModule(config1\u001b[39m=\u001b[39mconfig1, \n\u001b[0;32m     43\u001b[0m                         config2\u001b[39m=\u001b[39mconfig2, \n\u001b[0;32m     44\u001b[0m                         config3\u001b[39m=\u001b[39mconfig3, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m                         batch_size \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m,\n\u001b[0;32m     52\u001b[0m                         desired_score_list\u001b[39m=\u001b[39m[])\n\u001b[0;32m     54\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mGenerating molecules...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 55\u001b[0m graph_batch \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49mgenerate_mol_list(\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[23], line 410\u001b[0m, in \u001b[0;36mGenerationModule.generate_mol_list\u001b[1;34m(self, n_mol, n_threads)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[39mif\u001b[39;00m n_threads \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    409\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n_mol), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerating molecules\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 410\u001b[0m         mol_graph \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_single_molecule()\n\u001b[0;32m    411\u001b[0m         mol_list\u001b[39m.\u001b[39mappend(mol_graph)\n\u001b[0;32m    412\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m \n\u001b[0;32m    414\u001b[0m     \u001b[39m# Utilize ThreadPoolExecutor to parallelize the task\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 399\u001b[0m, in \u001b[0;36mGenerationModule.generate_single_molecule\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_single_molecule\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    387\u001b[0m     mol \u001b[39m=\u001b[39m MolGen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGNN1_model,\n\u001b[0;32m    388\u001b[0m                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGNN2_model,\n\u001b[0;32m    389\u001b[0m                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mGNN3_model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    397\u001b[0m                  score_list\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore_list,\n\u001b[0;32m    398\u001b[0m                  desired_score_list\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdesired_score_list)\n\u001b[1;32m--> 399\u001b[0m     mol\u001b[39m.\u001b[39;49mfull_generation()\n\u001b[0;32m    400\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecking_mode:\n\u001b[0;32m    401\u001b[0m         \u001b[39m# check validity of the molecule\u001b[39;00m\n\u001b[0;32m    402\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m mol\u001b[39m.\u001b[39mis_valid():\n",
      "Cell \u001b[1;32mIn[23], line 328\u001b[0m, in \u001b[0;36mMolGen.full_generation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m>\u001b[39m max_iter:\n\u001b[0;32m    327\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 328\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mone_step()\n\u001b[0;32m    329\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_intermidiate_states:\n",
      "Cell \u001b[1;32mIn[23], line 311\u001b[0m, in \u001b[0;36mMolGen.one_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    307\u001b[0m predicted_edge \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmultinomial(selected_edge_distribution, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    309\u001b[0m \u001b[39mif\u001b[39;00m predicted_edge \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    310\u001b[0m     \u001b[39m#Stop\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmol_graph \u001b[39m=\u001b[39m new_graph\n\u001b[0;32m    312\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    314\u001b[0m encoded_predicted_edge \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(prediction2\u001b[39m.\u001b[39msize(), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_graph' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# List the folders in the experiment path\n",
    "\n",
    "folders = os.listdir(experiment_path)\n",
    "for folder in folders:\n",
    "    if folder.startswith('GNN1'):\n",
    "        GNN1_path = experiment_path / folder \n",
    "    elif folder.startswith('GNN2'):\n",
    "        GNN2_path = experiment_path / folder\n",
    "    elif folder.startswith('GNN3'):\n",
    "        GNN3_path = experiment_path / folder\n",
    "\n",
    "# Read the config as a json\n",
    "\n",
    "config1_path = GNN1_path / 'parameters.json'\n",
    "config2_path = GNN2_path / 'parameters.json'\n",
    "config3_path = GNN3_path / 'parameters.json'\n",
    "\n",
    "with open(config1_path, 'r') as f:\n",
    "    config1 = json.load(f)\n",
    "with open(config2_path, 'r') as f:\n",
    "    config2 = json.load(f)\n",
    "with open(config3_path, 'r') as f:\n",
    "    config3 = json.load(f)\n",
    "\n",
    "# Open the models with the best loss on the validation set\n",
    "\n",
    "\n",
    "GNN1_path = load_best_models(GNN1_path)\n",
    "GNN2_path = load_best_models(GNN2_path)\n",
    "GNN3_path = load_best_models(GNN3_path)\n",
    "\n",
    "print(GNN1_path, GNN2_path, GNN3_path)\n",
    "\n",
    "encoding_size = 13\n",
    "\n",
    "\n",
    "edge_size = 3\n",
    "\n",
    "\n",
    "module = GenerationModule(config1=config1, \n",
    "                        config2=config2, \n",
    "                        config3=config3, \n",
    "                        encoding_size = encoding_size,\n",
    "                        edge_size = edge_size, \n",
    "                        pathGNN1=GNN1_path, \n",
    "                        pathGNN2=GNN2_path, \n",
    "                        pathGNN3=GNN3_path,\n",
    "                        encoding_type='charged',\n",
    "                        batch_size = 5,\n",
    "                        desired_score_list=[])\n",
    "\n",
    "print('Generating molecules...')\n",
    "graph_batch = module.generate_mol_list(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
