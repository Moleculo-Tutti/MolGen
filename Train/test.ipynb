{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "sys.path.append(parent_dir)\n",
    "from DataPipeline.dataset import ZincSubgraphDatasetStep, custom_collate_GNN1\n",
    "from DataPipeline.preprocessing import plot_graph\n",
    "\n",
    "from torch_geometric.utils import to_networkx\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_geometric_to_networkx(data):\n",
    "    \"\"\"\n",
    "    Convert a torch_geometric.data.Data object into a networkx.Graph object.\n",
    "\n",
    "    Args:\n",
    "    data (torch_geometric.data.Data): A PyTorch Geometric Data object representing the molecule.\n",
    "\n",
    "    Returns:\n",
    "    G (networkx.Graph): A NetworkX Graph object representing the molecule.\n",
    "    \"\"\"\n",
    "    # Modify node features to take the argmax, excluding the last element\n",
    "    if data.x.shape[1] > 1:\n",
    "        data.x = torch.argmax(data.x[:, :-1], dim=1).unsqueeze(1)\n",
    "\n",
    "    # Modify edge features to take the argmax\n",
    "    if data.edge_attr.shape[1] > 1:\n",
    "        data.edge_attr = torch.argmax(data.edge_attr, dim=1).unsqueeze(1)\n",
    "\n",
    "    G = to_networkx(data, node_attrs=['x'], edge_attrs=['edge_attr'])\n",
    "\n",
    "    for i in G.nodes:\n",
    "        x_attr = G.nodes[i]['x']\n",
    "        atomic_num = int(x_attr.item()) if hasattr(x_attr, 'item') else int(x_attr)\n",
    "        G.nodes[i]['atomic_num'] = atomic_num\n",
    "        del G.nodes[i]['x']\n",
    "\n",
    "    for i, j in G.edges:\n",
    "        edge_attr = G.edges[i, j]['edge_attr']\n",
    "        bond_type = edge_attr.item() if hasattr(edge_attr, 'item') else edge_attr\n",
    "        G.edges[i, j]['bond_type'] = bond_type\n",
    "        del G.edges[i, j]['edge_attr']\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "\n",
    "def custom_collate(batch):\n",
    "    sg_data_list = [item[0] for item in batch]\n",
    "    g_data_list = [item[1] for item in batch]\n",
    "    terminal_nodes_info_list = [item[2] for item in batch]\n",
    "    id_map_list = [item[3] for item in batch]\n",
    "    sg_data_batch = Batch.from_data_list(sg_data_list)\n",
    "    g_data_batch = Batch.from_data_list(g_data_list)\n",
    "    return sg_data_batch, g_data_batch, terminal_nodes_info_list, id_map_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch_geometric.data import Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset encoded with size 7\n"
     ]
    }
   ],
   "source": [
    "datapath = Path('..') / 'DataPipeline/data/preprocessed_graph_no_I_Br_P.pt'\n",
    "dataset = ZincSubgraphDatasetStep(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=0, collate_fn=custom_collate_GNN1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    6,     3,     3,  ..., 13727, 13718, 13717],\n",
      "        [    3,     6,     7,  ..., 13725, 13717, 13718]])\n",
      "tensor([[   15,    13,    13,  ..., 13681, 13678, 13680],\n",
      "        [   13,    15,    12,  ..., 13678, 13680, 13678]])\n",
      "tensor([[   19,    18,    18,  ..., 13997, 14000, 14004],\n",
      "        [   18,    19,    17,  ..., 14000, 14004, 14000]])\n",
      "tensor([[    3,     1,     1,  ..., 13693, 13701, 13698],\n",
      "        [    1,     3,     0,  ..., 13696, 13698, 13701]])\n",
      "tensor([[   26,    24,    24,  ..., 13901, 13901, 13897],\n",
      "        [   24,    26,    25,  ..., 13904, 13897, 13901]])\n",
      "tensor([[    8,     5,     5,  ..., 13740, 13758, 13757],\n",
      "        [    5,     8,     4,  ..., 13743, 13757, 13758]])\n",
      "tensor([[    2,     0,     0,  ..., 13445, 13447, 13442],\n",
      "        [    0,     2,     1,  ..., 13441, 13442, 13447]])\n",
      "tensor([[   11,     8,     8,  ..., 13923, 13923, 13925],\n",
      "        [    8,    11,    10,  ..., 13922, 13925, 13923]])\n",
      "tensor([[   10,     8,     8,  ..., 13536, 13529, 13527],\n",
      "        [    8,    10,     5,  ..., 13531, 13527, 13529]])\n",
      "tensor([[    2,     0,     0,  ..., 14155, 14169, 14168],\n",
      "        [    0,     2,     1,  ..., 14152, 14168, 14169]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(batch[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39medge_index)\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\goupi\\OneDrive - CentraleSupelec\\VSCODE\\Siena\\MolecularGen\\Molgen\\MolGen\\DataPipeline\\dataset.py:79\u001b[0m, in \u001b[0;36mZincSubgraphDatasetStep.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     77\u001b[0m mol_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(preprocessed_graph\u001b[39m.\u001b[39mx)\n\u001b[0;32m     78\u001b[0m num_steps \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(\u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m\u001b[39m*\u001b[39mmol_size \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m---> 79\u001b[0m subgraph, terminal_nodes, id_map \u001b[39m=\u001b[39m get_subgraph_with_terminal_nodes_step(preprocessed_graph, num_steps)\n\u001b[0;32m     80\u001b[0m subgraph\u001b[39m.\u001b[39mx[id_map[terminal_nodes[\u001b[39m0\u001b[39m]]][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     82\u001b[0m \u001b[39m#get the embedding of all the first element of terminal_nodes[1] and make them into a list to take the mean, if terminal_nodes[1] empty make torch.zeros(10)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goupi\\OneDrive - CentraleSupelec\\VSCODE\\Siena\\MolecularGen\\Molgen\\MolGen\\DataPipeline\\preprocessing.py:320\u001b[0m, in \u001b[0;36mget_subgraph_with_terminal_nodes_step\u001b[1;34m(data, num_steps, impose_edges)\u001b[0m\n\u001b[0;32m    318\u001b[0m     terminal_node_infos \u001b[39m=\u001b[39m (current, external_neighbors)\n\u001b[0;32m    319\u001b[0m     subgraph_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mlist\u001b[39m(subgraph_atoms), dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m--> 320\u001b[0m     subgraph_data \u001b[39m=\u001b[39m get_subgraph(data, subgraph_indices, id_map)\n\u001b[0;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m subgraph_data, terminal_node_infos, id_map\n\u001b[0;32m    324\u001b[0m \u001b[39mfor\u001b[39;00m neighbor \u001b[39min\u001b[39;00m neighbors:\n",
      "File \u001b[1;32mc:\\Users\\goupi\\OneDrive - CentraleSupelec\\VSCODE\\Siena\\MolecularGen\\Molgen\\MolGen\\DataPipeline\\preprocessing.py:160\u001b[0m, in \u001b[0;36mget_subgraph\u001b[1;34m(data, indices, id_map)\u001b[0m\n\u001b[0;32m    157\u001b[0m index_map \u001b[39m=\u001b[39m {old_index: id_map[old_index] \u001b[39mfor\u001b[39;00m old_index \u001b[39min\u001b[39;00m indices\u001b[39m.\u001b[39mtolist()}\n\u001b[0;32m    159\u001b[0m \u001b[39m# Extract node features\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m subgraph_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\u001b[39mlen\u001b[39;49m(index_map), data\u001b[39m.\u001b[39;49mx\u001b[39m.\u001b[39;49msize(\u001b[39m1\u001b[39;49m))\n\u001b[0;32m    161\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(indices)):\n\u001b[0;32m    162\u001b[0m     subgraph_x[index_map[indices[i]\u001b[39m.\u001b[39mitem()]] \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx[indices[i]]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch[0].edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [04:25<00:00,  1.15s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [1024, 7] at entry 0 and [597, 7] at entry 230",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(dataloader):\n\u001b[0;32m      5\u001b[0m     y_list\u001b[39m.\u001b[39mappend(batch[\u001b[39m1\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m y_list \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack(y_list)\n\u001b[0;32m      9\u001b[0m \u001b[39msum\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(y_list, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [1024, 7] at entry 0 and [597, 7] at entry 230"
     ]
    }
   ],
   "source": [
    "y_list = []\n",
    "\n",
    "for batch in tqdm(dataloader):\n",
    "\n",
    "    y_list.append(batch[1])\n",
    "y_list = torch.stack(y_list)\n",
    "\n",
    "\n",
    "sum = torch.sum(y_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = torch.cat(y_list, dim=0)\n",
    "sum = torch.sum(concat, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 82792.5859,  13852.4150,  11517.1660,   1634.7500,   2047.0001,\n",
       "           904.0834, 123369.0000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3506, 0.0587, 0.0488, 0.0069, 0.0087, 0.0038, 0.5225])\n"
     ]
    }
   ],
   "source": [
    "print(sum/len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(class_proportions):\n",
    "\n",
    "    # Compute the inverse of the class proportions\n",
    "    class_weights = 1 / class_proportions\n",
    "\n",
    "    # Normalize the class weights so they sum up to 1\n",
    "    class_weights_normalized = class_weights / class_weights.sum()\n",
    "\n",
    "    return class_weights_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "for batch in dataloader:\n",
    "    i += 1\n",
    "    data_list = Batch.to_data_list(batch[0])\n",
    "    g_data_list = Batch.to_data_list(batch[1])\n",
    "    network_subgraph = torch_geometric_to_networkx(data_list[0])\n",
    "    network_graph = torch_geometric_to_networkx(g_data_list[0])\n",
    "    id_map = batch[3][0]\n",
    "\n",
    "    plot_graph(network_subgraph, atom_conversion_type='onehot', encoding_type='reduced')\n",
    "    plot_graph(network_graph, atom_conversion_type='onehot', encoding_type='reduced', id_map=id_map)\n",
    "    print(batch[2])\n",
    "    print(id_map)\n",
    "\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
