{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "sys.path.append(parent_dir)\n",
    "from DataPipeline.dataset import ZincSubgraphDatasetStep, custom_collate_GNN1\n",
    "from Model.GNN1 import ModelWithEdgeFeatures\n",
    "from Model.metrics import pseudo_accuracy_metric, pseudo_recall_for_each_class, pseudo_precision_for_each_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(class_proportions):\n",
    "\n",
    "    # Compute the inverse of the class proportions\n",
    "    class_weights = 1 / class_proportions\n",
    "\n",
    "    # Normalize the class weights so they sum up to 1\n",
    "    class_weights_normalized = class_weights / class_weights.sum()\n",
    "\n",
    "    return class_weights_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_proportion = torch.Tensor([0.3506, 0.0587, 0.0488, 0.0069, 0.0087, 0.0038, 0.5225])\n",
    "class_weights = compute_class_weights(class_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset encoded with size 7\n"
     ]
    }
   ],
   "source": [
    "datapath = Path('..') / 'DataPipeline/data/preprocessed_graph_no_I_Br_P.pt'\n",
    "dataset = ZincSubgraphDatasetStep(data_path = datapath, GNN_type=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=0, collate_fn=custom_collate_GNN1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb2a8ea9aad45a68d7f9bb40b36a655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1845 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m number_reference_dict\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    115\u001b[0m     number_reference_dict[key] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m((\u001b[39mint\u001b[39m(value) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m%\u001b[39mmax_reference_dict[key])\n\u001b[1;32m--> 117\u001b[0m loss, avg_label_vector, avg_output_vector, avg_correct \u001b[39m=\u001b[39m train(dataloader, epoch)\n\u001b[0;32m    118\u001b[0m training_history \u001b[39m=\u001b[39m training_history\u001b[39m.\u001b[39mappend({\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: epoch, \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: loss, \u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m: mean_squared_error(avg_label_vector, avg_output_vector), \u001b[39m'\u001b[39m\u001b[39mavg_output_vector\u001b[39m\u001b[39m'\u001b[39m: avg_output_vector, \u001b[39m'\u001b[39m\u001b[39mavg_label_vector\u001b[39m\u001b[39m'\u001b[39m: avg_label_vector, \u001b[39m'\u001b[39m\u001b[39mavg_correct\u001b[39m\u001b[39m'\u001b[39m: avg_correct}, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    119\u001b[0m \u001b[39m#save the model(all with optimizer step, the loss ) every 5 epochs\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(loader, epoch)\u001b[0m\n\u001b[0;32m     41\u001b[0m avg_label_vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(encoding_size)  \u001b[39m# Initialize the average label vector\u001b[39;00m\n\u001b[0;32m     42\u001b[0m total_graphs_processed \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 46\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(progress_bar):\n\u001b[0;32m     47\u001b[0m     data \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[0;32m     48\u001b[0m     terminal_node_infos \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\tqdm\\notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 259\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    260\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    261\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    262\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\goupi\\.conda\\envs\\torch_geometric\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\goupi\\OneDrive - CentraleSupelec\\VSCODE\\Siena\\MolecularGen\\Molgen\\MolGen\\DataPipeline\\dataset.py:91\u001b[0m, in \u001b[0;36mZincSubgraphDatasetStep.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     88\u001b[0m subgraph\u001b[39m.\u001b[39mx[id_map[terminal_nodes[\u001b[39m0\u001b[39m]]][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     90\u001b[0m \u001b[39m#get the embedding of all the first element of terminal_nodes[1] and make them into a list to take the mean, if terminal_nodes[1] empty make torch.zeros(10)\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m label_gnn1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding_size)\n\u001b[0;32m     92\u001b[0m neighbor_atom_list \u001b[39m=\u001b[39m [neighbor[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m neighbor \u001b[39min\u001b[39;00m terminal_nodes[\u001b[39m1\u001b[39m]]\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(neighbor_atom_list) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "encoding_size = 7\n",
    "\n",
    "model = ModelWithEdgeFeatures(num_classes = encoding_size, in_channels=encoding_size, hidden_channels_list=[64, 128, 256, 512, 512], mlp_hidden_channels=512, edge_channels=4, use_dropout=False, size_info=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Set up the loss function for multiclass \n",
    " \n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "\n",
    "name = \"weighted_new_10-4\"\n",
    "\n",
    "# Training function\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "def train(loader, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    mse_sum = 0\n",
    "    num_correct = 0\n",
    "    num_correct_recall = torch.zeros(encoding_size)\n",
    "    num_correct_precision = torch.zeros(encoding_size)\n",
    "    count_per_class_recall = torch.zeros(encoding_size)\n",
    "    count_per_class_precision = torch.zeros(encoding_size)\n",
    "    progress_bar = tqdm_notebook(loader, desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "    avg_output_vector = np.zeros(encoding_size)  # Initialize the average output vector\n",
    "    avg_label_vector = np.zeros(encoding_size)  # Initialize the average label vector\n",
    "    total_graphs_processed = 0\n",
    "\n",
    "    \n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        data = batch[0]\n",
    "        terminal_node_infos = batch[1]\n",
    "        data = data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        terminal_node_infos = terminal_node_infos.to(device)\n",
    "\n",
    "        loss = criterion(out, terminal_node_infos)\n",
    "        num_correct += pseudo_accuracy_metric(out.detach().cpu(), terminal_node_infos.detach().cpu(), random=True)\n",
    "\n",
    "        recall_output = pseudo_recall_for_each_class(out.detach().cpu(), terminal_node_infos.detach().cpu(), random=True)\n",
    "        precision_output = pseudo_precision_for_each_class(out.detach().cpu(), terminal_node_infos.detach().cpu(), random=True)\n",
    "        num_correct_recall += recall_output[0]\n",
    "        num_correct_precision += precision_output[0]\n",
    "        count_per_class_recall += recall_output[1]\n",
    "        count_per_class_precision += precision_output[1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        loss_value = total_loss / (data.num_graphs * (progress_bar.last_print_n + 1))\n",
    "\n",
    "        # Compute MSE\n",
    "        mse = mean_squared_error(terminal_node_infos.detach().cpu(), out.detach().cpu())\n",
    "        mse_sum += mse * data.num_graphs\n",
    "        mse_value = mse_sum / (data.num_graphs * (progress_bar.last_print_n + 1))\n",
    "\n",
    "        # Update the average output vector\n",
    "        avg_output_vector += out.detach().cpu().numpy().mean(axis=0) * data.num_graphs\n",
    "        avg_label_vector += terminal_node_infos.detach().cpu().numpy().mean(axis=0) * data.num_graphs\n",
    "        total_graphs_processed += data.num_graphs\n",
    "        current_avg_output_vector = avg_output_vector / total_graphs_processed\n",
    "        current_avg_label_vector = avg_label_vector / total_graphs_processed\n",
    "        avg_correct = num_correct / total_graphs_processed\n",
    "        avg_correct_recall = num_correct_recall / count_per_class_recall\n",
    "        avg_correct_precision = num_correct_precision / count_per_class_precision\n",
    "        avg_f1 = 2 * (avg_correct_recall * avg_correct_precision) / (avg_correct_recall + avg_correct_precision)\n",
    "        progress_bar.set_postfix(loss=loss_value, mse=mse_value, avg_output_vector=current_avg_output_vector, \n",
    "                                 avg_label_vector=current_avg_label_vector, \n",
    "                                 avg_correct=avg_correct, num_correct=num_correct, \n",
    "                                 total_graphs_processed=total_graphs_processed, \n",
    "                                 avg_correct_precision=avg_correct_precision, \n",
    "                                 avg_correct_recall=avg_correct_recall, \n",
    "                                 avg_f1=avg_f1,\n",
    "                                 count_per_class_precision=count_per_class_precision,\n",
    "                                 count_per_class_recall=count_per_class_recall)\n",
    "\n",
    "\n",
    "    return total_loss / len(loader.dataset), current_avg_label_vector, current_avg_output_vector, avg_correct\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Create a dataframe to save the training history\n",
    "training_history = pd.DataFrame(columns=['epoch', 'loss', 'mse', 'avg_output_vector', 'avg_label_vector'])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "number_reference_dict = {'C' : '0', 'N' : '0', 'O' : '0', 'S' : '0', 'F' : '0', 'Cl' : '0', 'stop' : '0'}\n",
    "max_reference_dict = {'C' : 100, 'N' : 50, 'O' : 50, 'S' : 5, 'F' : 7, 'Cl' : 4, 'stop' : 500}\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "\n",
    "    # +1 in number_reference_dict\n",
    "    \n",
    "    for key, value in number_reference_dict.items():\n",
    "        number_reference_dict[key] = str((int(value) + 1)%max_reference_dict[key])\n",
    "\n",
    "    loss, avg_label_vector, avg_output_vector, avg_correct = train(dataloader, epoch)\n",
    "    training_history = training_history.append({'epoch': epoch, 'loss': loss, 'mse': mean_squared_error(avg_label_vector, avg_output_vector), 'avg_output_vector': avg_output_vector, 'avg_label_vector': avg_label_vector, 'avg_correct': avg_correct}, ignore_index=True)\n",
    "    #save the model(all with optimizer step, the loss ) every 5 epochs\n",
    "\n",
    "    save_every_n_epochs = 5\n",
    "    if (epoch) % save_every_n_epochs == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            # Add any other relevant information you want to save here\n",
    "        }\n",
    "        torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}_{name}.pt')\n",
    "        \n",
    "    #save the training history every encoding_size epochs\n",
    "    if epoch % 1 == 0:\n",
    "        training_history.to_csv(f\"training_history_{name}.csv\", index=False)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
