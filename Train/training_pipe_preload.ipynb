{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent_dir = os.path.dirname(cwd)\n",
    "sys.path.append(parent_dir)\n",
    "from DataPipeline.dataset import ZincPreloadDataset, custom_collate_passive_add_feature\n",
    "from Model.GNN1 import ModelWithEdgeFeatures\n",
    "from Model.metrics import pseudo_accuracy_metric, pseudo_recall_for_each_class, pseudo_precision_for_each_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(number_reference_dict, dir_path, custom_collate, batch_size, shuffle, num_workers):\n",
    "    dataset = ZincPreloadDataset(number_reference_dict, dir_path)\n",
    "    return DataLoader(dataset, batch_size=128, shuffle=True, num_workers=0, collate_fn=custom_collate_passive_add_feature)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader the batch named ..\\DataPipeline\\data\\prepared_new_dataset\\C\\C100000_0.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1/7 [00:26<02:37, 26.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader the batch named ..\\DataPipeline\\data\\prepared_new_dataset\\N\\N100000_0.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [00:50<02:04, 24.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader the batch named ..\\DataPipeline\\data\\prepared_new_dataset\\O\\O100000_0.zst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2/7 [01:10<02:56, 35.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 114\u001b[0m\n\u001b[0;32m    108\u001b[0m max_reference_dict \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m300\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mN\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mO\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m18\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mF\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m15\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCl\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m8\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m'\u001b[39m : \u001b[39m500\u001b[39m}\n\u001b[0;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m--> 114\u001b[0m     dataset \u001b[39m=\u001b[39m ZincPreloadDataset(number_reference_dict\u001b[39m=\u001b[39;49mnumber_reference_dict, data_dir\u001b[39m=\u001b[39;49mdir_path)\n\u001b[0;32m    115\u001b[0m     loader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mcustom_collate_passive_add_feature)\n\u001b[0;32m    117\u001b[0m     \u001b[39m# +1 in number_reference_dict\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goupi\\OneDrive - CentraleSupelec\\VSCODE\\Siena\\MolecularGen\\Molgen\\MolGen\\DataPipeline\\dataset.py:253\u001b[0m, in \u001b[0;36mZincPreloadDataset.__init__\u001b[1;34m(self, number_reference_dict, data_dir)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, number_reference_dict, data_dir):\n\u001b[1;32m--> 253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_list \u001b[39m=\u001b[39m load_all_and_merge(number_reference_dict, data_dir)\n\u001b[0;32m    254\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_list[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[0;32m    255\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDataset encoded with size \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoding_size))\n",
      "File \u001b[1;32mc:\\Users\\goupi\\OneDrive - CentraleSupelec\\VSCODE\\Siena\\MolecularGen\\Molgen\\MolGen\\DataPipeline\\dataset.py:243\u001b[0m, in \u001b[0;36mload_all_and_merge\u001b[1;34m(number_reference_dict, data_dir)\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mloader the batch named \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(batch_path))\n\u001b[0;32m    241\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m     graph_batch \u001b[39m=\u001b[39m load_compressed_batch(batch_path)\n\u001b[0;32m    244\u001b[0m     all_graphs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m graph_batch\u001b[39m.\u001b[39mto_data_list()\n\u001b[0;32m    246\u001b[0m \u001b[39m#shuffling the list of graphs\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\goupi\\OneDrive - CentraleSupelec\\VSCODE\\Siena\\MolecularGen\\Molgen\\MolGen\\DataPipeline\\dataset.py:222\u001b[0m, in \u001b[0;36mload_compressed_batch\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m    220\u001b[0m dctx \u001b[39m=\u001b[39m zstd\u001b[39m.\u001b[39mZstdDecompressor()\n\u001b[0;32m    221\u001b[0m decompressor \u001b[39m=\u001b[39m dctx\u001b[39m.\u001b[39mdecompressobj()\n\u001b[1;32m--> 222\u001b[0m decompressed_data \u001b[39m=\u001b[39m decompressor\u001b[39m.\u001b[39;49mdecompress(f\u001b[39m.\u001b[39;49mread())\n\u001b[0;32m    223\u001b[0m decompressed_buffer \u001b[39m=\u001b[39m BytesIO(decompressed_data)\n\u001b[0;32m    224\u001b[0m graph_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(decompressed_buffer)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "encoding_size = 7\n",
    "\n",
    "model = ModelWithEdgeFeatures(num_classes = encoding_size, in_channels=encoding_size + 1, hidden_channels_list=[64, 128, 256, 512, 512], mlp_hidden_channels=512, edge_channels=4, use_dropout=False, size_info=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Set up the loss function for multiclass \n",
    " \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "name = 'new_balanced_10-4'\n",
    "\n",
    "# Training function\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "def train(loader, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    mse_sum = 0\n",
    "    num_correct = 0\n",
    "    num_correct_recall = torch.zeros(encoding_size)\n",
    "    num_correct_precision = torch.zeros(encoding_size)\n",
    "    count_per_class_recall = torch.zeros(encoding_size)\n",
    "    count_per_class_precision = torch.zeros(encoding_size)\n",
    "    progress_bar = tqdm_notebook(loader, desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "    avg_output_vector = np.zeros(encoding_size)  # Initialize the average output vector\n",
    "    avg_label_vector = np.zeros(encoding_size)  # Initialize the average label vector\n",
    "    total_graphs_processed = 0\n",
    "\n",
    "    \n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        data = batch[0]\n",
    "        terminal_node_infos = batch[1]\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        terminal_node_infos = terminal_node_infos.to(device)\n",
    "\n",
    "        loss = criterion(out, terminal_node_infos)\n",
    "        num_correct += pseudo_accuracy_metric(out.detach().cpu(), terminal_node_infos.detach().cpu(), random=True)\n",
    "\n",
    "        recall_output = pseudo_recall_for_each_class(out.detach().cpu(), terminal_node_infos.detach().cpu(), random=True)\n",
    "        precision_output = pseudo_precision_for_each_class(out.detach().cpu(), terminal_node_infos.detach().cpu(), random=True)\n",
    "        num_correct_recall += recall_output[0]\n",
    "        num_correct_precision += precision_output[0]\n",
    "        count_per_class_recall += recall_output[1]\n",
    "        count_per_class_precision += precision_output[1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        loss_value = total_loss / (data.num_graphs * (progress_bar.last_print_n + 1))\n",
    "\n",
    "        # Compute MSE\n",
    "        mse = mean_squared_error(terminal_node_infos.detach().cpu(), out.detach().cpu())\n",
    "        mse_sum += mse * data.num_graphs\n",
    "        mse_value = mse_sum / (data.num_graphs * (progress_bar.last_print_n + 1))\n",
    "\n",
    "        # Update the average output vector\n",
    "        avg_output_vector += out.detach().cpu().numpy().mean(axis=0) * data.num_graphs\n",
    "        avg_label_vector += terminal_node_infos.detach().cpu().numpy().mean(axis=0) * data.num_graphs\n",
    "        total_graphs_processed += data.num_graphs\n",
    "        current_avg_output_vector = avg_output_vector / total_graphs_processed\n",
    "        current_avg_label_vector = avg_label_vector / total_graphs_processed\n",
    "        avg_correct = num_correct / total_graphs_processed\n",
    "        avg_correct_recall = num_correct_recall / count_per_class_recall\n",
    "        avg_correct_precision = num_correct_precision / count_per_class_precision\n",
    "        avg_f1 = 2 * (avg_correct_recall * avg_correct_precision) / (avg_correct_recall + avg_correct_precision)\n",
    "        progress_bar.set_postfix(loss=loss_value, mse=mse_value, avg_output_vector=current_avg_output_vector, \n",
    "                                 avg_label_vector=current_avg_label_vector, \n",
    "                                 avg_correct=avg_correct, num_correct=num_correct, \n",
    "                                 total_graphs_processed=total_graphs_processed, \n",
    "                                 avg_correct_precision=avg_correct_precision, \n",
    "                                 avg_correct_recall=avg_correct_recall, \n",
    "                                 avg_f1=avg_f1,\n",
    "                                 count_per_class_precision=count_per_class_precision,\n",
    "                                 count_per_class_recall=count_per_class_recall)\n",
    "\n",
    "\n",
    "    return total_loss / len(loader.dataset), current_avg_label_vector, current_avg_output_vector, avg_correct\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Create a dataframe to save the training history\n",
    "training_history = pd.DataFrame(columns=['epoch', 'loss', 'mse', 'avg_output_vector', 'avg_label_vector'])\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "dir_path = Path('..') / 'DataPipeline' / 'data' / 'prepared_new_dataset'\n",
    "\n",
    "number_reference_dict = {'C' : '0', 'N' : '0', 'O' : '0', 'S' : '0', 'F' : '0', 'Cl' : '0', 'stop' : '0'}\n",
    "max_reference_dict = {'C' : 300, 'N' : 100, 'O' : 100, 'S' : 18, 'F' : 15, 'Cl' : 8, 'stop' : 500}\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "   \n",
    "    dataset = ZincPreloadDataset(number_reference_dict=number_reference_dict, data_dir=dir_path)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=custom_collate_passive_add_feature)\n",
    "\n",
    "    # +1 in number_reference_dict\n",
    "    \n",
    "    for key, value in number_reference_dict.items():\n",
    "        number_reference_dict[key] = str((int(value) + 1)%max_reference_dict[key])\n",
    "\n",
    "    loss, avg_label_vector, avg_output_vector, avg_correct = train(loader, epoch)\n",
    "    training_history = training_history.append({'epoch': epoch, 'loss': loss, 'mse': mean_squared_error(avg_label_vector, avg_output_vector), 'avg_output_vector': avg_output_vector, 'avg_label_vector': avg_label_vector, 'avg_correct': avg_correct}, ignore_index=True)\n",
    "    #save the model(all with optimizer step, the loss ) every 5 epochs\n",
    "\n",
    "    save_every_n_epochs = 5\n",
    "    if (epoch) % save_every_n_epochs == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            # Add any other relevant information you want to save here\n",
    "        }\n",
    "        torch.save(checkpoint, f'checkpoint_epoch_{epoch+1}_{name}.pt')\n",
    "        \n",
    "    #save the training history every encoding_size epochs\n",
    "    if epoch % 1 == 0:\n",
    "        training_history.to_csv(f\"training_history_{name}.csv\", index=False)\n",
    "    print(f'Epoch: {epoch}, Loss: {loss:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
